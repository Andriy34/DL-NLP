{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e194f8f-a903-4f3c-9a96-400f4fcd324c",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf97aeb-bf45-426f-bc9e-4693e26af724",
   "metadata": {},
   "source": [
    "Task 0. Execute the notebook and complete listed exercises (between CODE_START and CODE_END blocks).\n",
    "\n",
    "Завдання 0. Виконайте блокнот (ноутбук) та виконайте вказані вправи (між блоками CODE_START та CODE_END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864b019-3f1f-4a2d-a095-7572f210b566",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19275bd-b8f8-4bcc-bcdc-6aac942e5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from nltk) (1.5.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.2.0 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba58419-ca18-46bb-b708-2e0ba29ea7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/boykosvitlana/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a408d8-850e-499f-bc3e-39e0e85fdf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8096f2a-a3f5-40fa-ac1e-5f7151781ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da61d5f-f1b8-4ffb-bec8-65f06f691570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc7c3072-25c9-4ddf-bb4d-e6f2955143f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/boykosvitlana/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5e9aca-84db-44e2-bdf4-0e7841dbfc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e19b7-2f77-46da-bde3-dd2b1b96671b",
   "metadata": {},
   "source": [
    "### WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0cd2fc-8c6c-4914-aa08-f010284158b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/boykosvitlana/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ba0cbe-1e4b-4c5e-af38-f6c097161e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "lemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word_synset = wn.synsets(\"car\")\n",
    "print(\"synsets:\", word_synset)\n",
    "print(\"lemma names:\", word_synset[0].lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7f3151-3d75-4a72-a5ca-7556767e9501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39d8e2fd-a127-4853-aa73-f61531a6f561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2f70ac2-53b9-4c80-ae3b-effb9c442083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wheeled vehicle adapted to the rails of railroad'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3fee2e-06f0-47db-b005-3e0cae3415fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three cars had jumped the rails']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bf15-0aaa-48b8-9efc-6a2d8a23c212",
   "metadata": {},
   "source": [
    "### Hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b780d6a5-3565-46a4-919f-b3b2eee98859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('sport_utility.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('ambulance.n.01'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('used-car.n.01'),\n",
       " Synset('bus.n.04')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e13b012c-1885-41ac-9203-f775c5b50e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de04c05e-8437-459d-8c5a-2afd4d549bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n"
     ]
    }
   ],
   "source": [
    "tree = wn.synsets(\"tree\")[0]\n",
    "paths = tree.hypernym_paths()\n",
    "for p in paths:\n",
    "  print([synset.name() for synset in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa51d2-af91-4747-a3b6-4c8a9da19d53",
   "metadata": {},
   "source": [
    "### Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f547be6-662c-4b93-82ae-fa7149c1f66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('limb.n.02'),\n",
       " Synset('trunk.n.01'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('burl.n.02')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be705a-5503-4106-a1ce-73b07881db83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582dc840-2f15-4404-8fe3-e9ded9abbb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5cc01-2b59-4bc5-b058-7dc701bc9941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a552fdea-b623-45ff-9133-76afccb5157d",
   "metadata": {},
   "source": [
    "### Holonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b76a345-17df-4bde-8c36-8b670fae9764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.member_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e658dc0e-2a01-42fd-9ce9-909329915835",
   "metadata": {},
   "source": [
    "### Lemmatization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27ddf7be-1806-49ca-9754-6594381a18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]  # Припускаємо, що tweet_tokens вже визначено"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a97f9977-7337-420a-8be2-3281ba304c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "125c5422-7427-4bf5-ae12-32fa16531f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    # Отримуємо частини мови для кожного токена\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    for token, tag in tagged_tokens:\n",
    "        # Конвертуємо POS-тег у формат WordNet\n",
    "        if tag.startswith('NN'):  # Іменник\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):  # Дієслово\n",
    "            pos = 'v'\n",
    "        else:  # Всі інші - прикметник (за замовчуванням)\n",
    "            pos = 'a'\n",
    "        \n",
    "        # Лематизуємо слово з відповідною частиною мови\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        lemmatized_sentence.append(lemmatized_token)\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Приклад використання\n",
    "\n",
    "lemmatized = lemmatize_sentence(tokens)\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060e34a-e273-4d2a-ae6e-71592308bfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea56daf-9586-40de-aba5-364f19eab715",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5131616d-3fc8-4be0-a191-76e12c994f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/boykosvitlana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be2d1f2b-1e7f-442c-90f9-958a83f2d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "ain\n",
      "all\n",
      "am\n",
      "an\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce87ab65-f3c6-407d-abb1-0728cfca034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "      # CODE_START\n",
    "      # Видаляємо URLs та згадки (@username)\n",
    "        token = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub(r'@\\w+', '', token)\n",
    "        \n",
    "        # Видаляємо пунктуацію та порожні токени\n",
    "        token = token.lower().strip()\n",
    "        if token in string.punctuation or not token:\n",
    "            continue\n",
    "            \n",
    "        # Пропускаємо стоп-слова\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "            \n",
    "        # Конвертуємо POS-теги\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "            \n",
    "        # Лематизуємо та додаємо до результату\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        cleaned_tokens.append(lemmatized_token)\n",
    "    \n",
    "      # CODE_END\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4dcc34a-c6e5-4425-a235-0992afc0847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26790b3-8707-402e-9053-35b75778c735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbf05b32-9bd5-4100-a0db-396211ac5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обробка позитивних та негативних твітів\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "# Завантажуємо та токенізуємо твіти\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "# Обробляємо токени\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acb6463a-f388-4014-962e-8888ec826be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d47b328-8e3e-4ffa-be18-95acb2177282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 найпоширеніших слів у позитивних твітах:\n",
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 383), ('follow', 362), ('love', 336), ('...', 290), ('good', 283), ('get', 269), ('thank', 258)]\n",
      "\n",
      "10 найпоширеніших слів у негативних твітах:\n",
      "[(':(', 4585), (':-(', 501), ('...', 332), ('get', 328), ('miss', 302), ('go', 280), ('please', 275), ('want', 246), ('like', 219), ('♛', 210)]\n"
     ]
    }
   ],
   "source": [
    "# аналіз частоти слів \n",
    "from nltk import FreqDist \n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    all_words = []\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            all_words.append(token)\n",
    "    return all_words\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)\n",
    "\n",
    "# Аналіз частот для позитивних твітів\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(\"10 найпоширеніших слів у позитивних твітах:\")\n",
    "print(freq_dist_pos.most_common(10))\n",
    "\n",
    "# Аналіз частот для негативних твітів\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "print(\"\\n10 найпоширеніших слів у негативних твітах:\")\n",
    "print(freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16ba54-5b31-4f68-8107-db9adaa2b72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "655d7403-ade5-4bff-b5f0-9b65d39e0a8d",
   "metadata": {},
   "source": [
    "\n",
    "Task 1. Change the code so it removes hashtags during pre-processing. (E.g. #Ukraine).\n",
    "\n",
    "Завдання 1. Змініть код так, щоб під час попередньої обробки видалялися хештеги (наприклад, #Ukraine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a557031-fc96-47b6-aede-b807feefb8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')   # список стоп-слів англійської мови\n",
    "    lemmatizer = WordNetLemmatizer()          # Ініціалізуємо лематизатор\n",
    "    \n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Зберігаємо слова з хештегів (видаляємо тільки #)\n",
    "        token = re.sub(r'#', '', token)  # видаляємо символ #\n",
    "        \n",
    "        # Видаляємо URLs(починаються з http/https) та згадки користувачів(@username)\n",
    "        token = re.sub(r'http[s]?://\\S+', '', token)\n",
    "        token = re.sub(r'@\\w+', '', token)\n",
    "        \n",
    "        # Нормалізація\n",
    "        token = token.lower().strip() # Приводимо до нижнього регістру і видаляємо зайві пробіли\n",
    "        \n",
    "        # Пропускаємо порожні токени, пунктуацію та стоп-слова\n",
    "        if not token or token in string.punctuation or token in stop_words:\n",
    "            continue\n",
    "            \n",
    "        # Визначаємо частину мови для лематизації\n",
    "        if tag.startswith('NN'):        # Іменник\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):      # Дієслово\n",
    "            pos = 'v'\n",
    "        else:                      # Всі інші - прикметник (за замовчуванням)\n",
    "            pos = 'a'\n",
    "            \n",
    "        # Лематизуємо слово з урахуванням частини мови\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        cleaned_tokens.append(lemmatized_token) # Додаємо результат до списку\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ce2f4c3-dbf8-4eee-861e-d59d15efad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ukraine', 'beautiful', 'travel']\n"
     ]
    }
   ],
   "source": [
    "tweet = [\"#Ukraine\", \"is\", \"beautiful\", \"!\", \"@user\", \"#travel\", \"http://example.com\"]\n",
    "print(process_tokens(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f685480-689f-4325-9820-1c97805d3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebded34-858b-4336-a951-eb87a3ed870d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a11bcaf8-8d29-4b0d-9eba-36973caea130",
   "metadata": {},
   "source": [
    "\n",
    "Task 2. Modify process_tokens() so that instead of using lemmatizer.lemmatize(), it will use WordNet synsets.\n",
    "\n",
    "Завдання 2. Модифікуйте функцію process_tokens() так, щоб замість використання lemmatizer.lemmatize() вона використовувала WordNet synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a580fa43-32d9-4430-a9f1-a13a541ff67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet  # Додатковий імпорт для роботи з synsets\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "    \"\"\"\n",
    "    Модифікована версія функції, яка використовує WordNet synsets замість лематизації.\n",
    "    Вибір основної форми слова відбувається на основі першого synset з WordNet.\n",
    "    \"\"\"\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # 1. Видалення небажаних елементів (як у попередній версії)\n",
    "        token = re.sub(r'#', '', token)\n",
    "        token = re.sub(r'http[s]?://\\S+', '', token)\n",
    "        token = re.sub(r'@\\w+', '', token)\n",
    "        token = token.lower().strip()\n",
    "        \n",
    "        # 2. Фільтрація (як у попередній версії)\n",
    "        if not token or token in string.punctuation or token in stop_words:\n",
    "            continue\n",
    "            \n",
    "        # 3. Визначення частини мови для WordNet\n",
    "        if tag.startswith('NN'):  # Іменник\n",
    "            pos = wordnet.NOUN\n",
    "        elif tag.startswith('VB'):  # Дієслово\n",
    "            pos = wordnet.VERB\n",
    "        elif tag.startswith('JJ'):  # Прикметник\n",
    "            pos = wordnet.ADJ\n",
    "        elif tag.startswith('RB'):  # Прислівник\n",
    "            pos = wordnet.ADV\n",
    "        else:  # За замовчуванням - іменник\n",
    "            pos = wordnet.NOUN\n",
    "            \n",
    "        # 4. Отримання synsets для токена\n",
    "        synsets = wordnet.synsets(token, pos=pos)\n",
    "        \n",
    "        if synsets:  # Якщо знайшли synset\n",
    "            # Беремо перше (найчастіше вживане) значення\n",
    "            lemma_name = synsets[0].lemmas()[0].name()\n",
    "            cleaned_tokens.append(lemma_name)\n",
    "        else:\n",
    "            # Якщо synset не знайдено, використовуємо оригінальне слово\n",
    "            cleaned_tokens.append(token)\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97c7f149-0657-48a3-bd7f-ff9a71463e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'happily', 'beautiful', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tweet = [\"running\", \"happily\", \"beautiful\", \"dogs\"]\n",
    "print(process_tokens(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c11285-d402-4c2f-9e39-25548a0dcc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d81ed18-2580-4618-9d38-b3e6290e7fba",
   "metadata": {},
   "source": [
    "\n",
    "Task 3. Let’s suppose that semantic distance between words is the distance to the common semantic parent (hypernym). Write a function that will compute this distance between two words.\n",
    "\n",
    "Завдання 3. Припустимо, що семантична відстань між словами — це відстань до спільного семантичного \"parent\" (гіперніма). Напишіть функцію, яка обчислюватиме цю відстань між двома словами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "002da6bb-2853-484d-a5f1-2275d7429bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функція для обчислення семантичної відстані між словами\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import product\n",
    "\n",
    "def semantic_distance(word1, word2):\n",
    "\n",
    "    # Отримуємо всі можливі значення слів (synsets) з WordNet\n",
    "    synsets1 = wordnet.synsets(word1)\n",
    "    synsets2 = wordnet.synsets(word2)\n",
    "\n",
    "    # Перевірка наявності слів у WordNet\n",
    "    if not synsets1 or not synsets2:\n",
    "        return None, None  # Якщо одне з слів не знайдено\n",
    "    \n",
    "    min_distance = float('inf')  # ініціалізація мінімальної відстані\n",
    "    best_hypernym = None         # зберігаємо найближчий спільний гіпернім\n",
    "\n",
    "    \n",
    "    # Перебираємо всі можливі пари synset для обох слів\n",
    "    for syn1, syn2 in product(synsets1, synsets2):\n",
    "        # Знаходимо спільні гіперніми\n",
    "        hypernyms1 = {h for h in syn1.common_hypernyms(syn2)}\n",
    "        hypernyms2 = {h for h in syn2.common_hypernyms(syn1)}\n",
    "        common_hypernyms = hypernyms1.union(hypernyms2)\n",
    "        \n",
    "        if not common_hypernyms:   # Якщо спільних гіпернімів немає - пропускаємо\n",
    "            continue  \n",
    "        \n",
    "        # обчислюємо відстань від першого та другого слова до гіперніма\n",
    "        for hypernym in common_hypernyms:\n",
    "            distance1 = syn1.shortest_path_distance(hypernym)\n",
    "            distance2 = syn2.shortest_path_distance(hypernym)\n",
    "            \n",
    "            if distance1 is None or distance2 is None:\n",
    "                continue  # Не знайшли шлях\n",
    "\n",
    "            # Загальна відстань - сума двох шляхів\n",
    "            total_distance = distance1 + distance2\n",
    "\n",
    "            # Зберігаємо найкоротший знайдений шлях\n",
    "            if total_distance < min_distance:\n",
    "                min_distance = total_distance\n",
    "                best_hypernym = hypernym\n",
    "\n",
    "    # Повертаємо результат (якщо знайшли хоча б один шлях)\n",
    "    return (min_distance, best_hypernym) if min_distance != float('inf') else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "efbfd247-cd40-491d-9f20-3bfba3c563e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Відстань між 'dog' та 'cat': 4\n",
      "Спільний гіпернім: carnivore.n.01\n",
      "\n",
      "Відстань між 'car' та 'bicycle': 2\n",
      "Спільний гіпернім: wheeled_vehicle.n.01\n",
      "\n",
      "Відстань між 'apple' та 'orange': 3\n",
      "Спільний гіпернім: edible_fruit.n.01\n",
      "\n",
      "Відстань між 'computer' та 'banana': 7\n",
      "Спільний гіпернім: organism.n.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Тестуємо функцію\n",
    "# Тестові пари слів\n",
    "word_pairs = [\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"car\", \"bicycle\"),\n",
    "    (\"apple\", \"orange\"),\n",
    "    (\"computer\", \"banana\")\n",
    "]\n",
    "\n",
    "for w1, w2 in word_pairs:\n",
    "    distance, hypernym = semantic_distance(w1, w2)\n",
    "    if distance is not None:\n",
    "        print(f\"Відстань між '{w1}' та '{w2}': {distance}\")\n",
    "        print(f\"Спільний гіпернім: {hypernym.name()}\")\n",
    "    else:\n",
    "        print(f\"Не знайдено спільного гіперніма для '{w1}' та '{w2}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e1626-1929-4b9d-80da-4284b550b3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bcaf8-d643-43d8-b19a-6a902cf927eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb72d2-514e-4399-89e6-19a363d3cdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677da018-ddfc-43a0-bbf1-951c94b86867",
   "metadata": {},
   "source": [
    " # Lab 4 (Naive Bayes classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e1f12-14ca-4b9a-a11c-04bd896f202a",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d0ad9-d53c-47fe-9753-a3c556b76679",
   "metadata": {},
   "source": [
    "Task 0. Execute the notebook.\n",
    "\n",
    "Завдання 0. Виконайте ноутбук."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b00ab674-2bb2-467a-a107-206999dee882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a469e35a-fd4e-48dc-89e7-5617796f21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(tokens):\n",
    "    return dict([token, True] for token in tokens)\n",
    "    \n",
    "def get_tweets_for_model(cleaned_tokens_list):   \n",
    "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cffdeca2-d787-4be3-8c4e-0f76f2a1d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6188a21-8cfb-490d-b432-bd535675042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2046.9 : 1.0\n",
      "                      :) = True           Positi : Negati =   1012.1 : 1.0\n",
      "                followed = True           Negati : Positi =     26.5 : 1.0\n",
      "                     sad = True           Negati : Positi =     24.0 : 1.0\n",
      "                follower = True           Positi : Negati =     22.6 : 1.0\n",
      "                    glad = True           Positi : Negati =     20.1 : 1.0\n",
      "                    poor = True           Negati : Positi =     18.0 : 1.0\n",
      "               community = True           Positi : Negati =     16.7 : 1.0\n",
      "                    blog = True           Positi : Negati =     14.6 : 1.0\n",
      "                   didnt = True           Negati : Positi =     14.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dcc789ba-8ed5-4629-a13f-60d0d89cabf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/boykosvitlana/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "126a7bca-e8d1-4371-acd8-e9af4e02d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"the service was so bad\"\n",
    "\n",
    "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(get_token_dict(custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3438e273-4041-4621-ab32-779b0dae8fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad :  Negative\n",
      "service is bad :  Negative\n",
      "service is really bad :  Negative\n",
      "service is so terrible :  Negative\n",
      "great service :  Positive\n",
      "they stole my money :  Negative\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    custom_tokens = process_tokens(word_tokenize(text))\n",
    "    return classifier.classify(get_token_dict(custom_tokens))\n",
    "\n",
    "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\n",
    "for t in texts:\n",
    "    print(t, \": \", get_sentiment(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f013439-fb61-4267-b53e-6f54e597f122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b8b42-5cac-474c-8945-2e8e4309591c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8511dcb-4e15-43b7-bbf3-c490e236c8e2",
   "metadata": {},
   "source": [
    "Task 1. Re-train the classifier on a different set of data. For instance, use a dataset from HuggingFace or Kaggle.\n",
    "\n",
    "Завдання 1. Повторно натренуйте класифікатор на іншому наборі даних. Наприклад, використайте датасет із HuggingFace або Kaggle.\n",
    "\n",
    "Examples are:\n",
    "\n",
    "https://huggingface.co/datasets/gxb912/large-twitter-tweets-sentiment\n",
    "\n",
    "https://huggingface.co/datasets/kaenova/hotel-sentiment\n",
    "\n",
    "https://huggingface.co/datasets/mltrev23/financial-sentiment-analysis\n",
    "\n",
    "https://huggingface.co/datasets/rohith2812/stanford-sentiment-treebank-dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis\n",
    "\n",
    "https://www.kaggle.com/datasets/charunisa/chatgpt-sentiment-analysis\n",
    "\n",
    "Try testing the classifier with different kinds of text - e.g. Reddit comments, tweets, whatever.\n",
    "\n",
    "Спробуйте протестувати класифікатор на різних типах тексту — наприклад, коментарі з Reddit, твіти тощо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2e72d53-1725-44fb-8a21-c696434138bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a9c3cd8-c1d5-4c22-93dc-c571ab1c2ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BofA previews Netflixs NFLX Q3 Earnings Tues 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I scooped a couple of shares this morning at a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Im streaming ES Futures using Bookmap on youtu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CF taking some off here close to 19150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No change to this position is still bullish st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Good week hot_beverage chart_increasing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Netflix up big That is massive for the entire ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Short VOL oddly the best trade of 22 fire Anot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>to watch before the bell bank US to kick off s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Looks like the last time had truly bad earning...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  label\n",
       "0     BofA previews Netflixs NFLX Q3 Earnings Tues 0...      0\n",
       "1     I scooped a couple of shares this morning at a...      0\n",
       "2     Im streaming ES Futures using Bookmap on youtu...      0\n",
       "3                CF taking some off here close to 19150      1\n",
       "4     No change to this position is still bullish st...      0\n",
       "...                                                 ...    ...\n",
       "9995           Good week hot_beverage chart_increasing       1\n",
       "9996  Netflix up big That is massive for the entire ...     -1\n",
       "9997  Short VOL oddly the best trade of 22 fire Anot...      0\n",
       "9998  to watch before the bell bank US to kick off s...      0\n",
       "9999  Looks like the last time had truly bad earning...     -1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\", sep=',') \n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea38e1-f638-4558-9195-e485d512f191",
   "metadata": {},
   "source": [
    "Створимо функцію для очищення та лематизації токенів "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7db87430-18ef-49c6-825a-550254fa19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bofa', 'preview', 'netflixs', 'nflx', 'q3', 'earnings', 'tues', '0', '...']\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    # Токенізація (розділяєм твіти на окремі слова)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # приводимо слова до нормальної форми та видаляємо непотрібні елементи(список стоп - слів)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        # Видаляємо посилання, @згадки та спецсимволи\n",
    "        token = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub(r'@\\w+', '', token)\n",
    "        \n",
    "        if token not in string.punctuation and token.lower() not in stop_words and token != '':\n",
    "            # Конвертуємо теги для лематизатора\n",
    "            if tag.startswith('NN'):     # Якщо тег починається з 'NN' - іменник (noun)\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):     # 'VB' - дієслово (verb)\n",
    "                pos = 'v'\n",
    "            else:              \n",
    "                pos = 'a'      # прикметник (adjective)\n",
    "                \n",
    "            token = lemmatizer.lemmatize(token.lower(), pos)\n",
    "            cleaned_tokens.append(token)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Тестуємо функцію\n",
    "print(process_text(\"BofA previews Netflixs NFLX Q3 Earnings Tues 0...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621020e-1a19-44df-b0f7-62f48f7d7662",
   "metadata": {},
   "source": [
    "Обробляємо всі твіти та створюємо словники для навчання:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6ac56a04-31a6-4318-b984-8c0d89d9af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Розділяємо на позитивні/негативні \n",
    "positive_tweets = data[data['label'] == 1]['tweet']\n",
    "negative_tweets = data[data['label'] == -1]['tweet']\n",
    "neutral_tweets = data[data['label'] == 0]['tweet']  \n",
    "\n",
    "# Обробляємо твіти\n",
    "positive_cleaned = [process_text(tweet) for tweet in positive_tweets]\n",
    "negative_cleaned = [process_text(tweet) for tweet in negative_tweets]\n",
    "\n",
    "# Функція для створення словників токенів\n",
    "def get_tokens_dict(cleaned_tokens_list):\n",
    "    return [dict([token, True] for token in tokens) for tokens in cleaned_tokens_list]\n",
    "\n",
    "positive_for_model = get_tokens_dict(positive_cleaned)\n",
    "negative_for_model = get_tokens_dict(negative_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5581004-5ed1-464c-a967-3eb0d7198fd8",
   "metadata": {},
   "source": [
    "Розділяємо дані на тренувальні та тестові:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a9a01ee7-65df-464a-b82d-458aafcb6574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8227091633466136\n",
      "Most Informative Features\n",
      "                facebook = True           Negati : Positi =     30.9 : 1.0\n",
      "                     fix = True           Negati : Positi =     15.8 : 1.0\n",
      "                     fed = True           Negati : Positi =     15.4 : 1.0\n",
      "               watchlist = True           Positi : Negati =     14.9 : 1.0\n",
      "                    lose = True           Negati : Positi =     14.6 : 1.0\n",
      "                   covid = True           Negati : Positi =     13.7 : 1.0\n",
      "               instagram = True           Negati : Positi =     13.2 : 1.0\n",
      "                   email = True           Negati : Positi =     13.0 : 1.0\n",
      "                    fail = True           Negati : Positi =     13.0 : 1.0\n",
      "                     sue = True           Negati : Positi =     12.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Створюємо негативні та позитивні твіти і обʼєднуємо їх в датасет\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_for_model]\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "import random\n",
    "# Перемішуємо датасет для випадкового розподілу твітів\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Розділяємо дані на навчальні (70%) та тестові (30%)\n",
    "train_data = dataset[:int(0.7*len(dataset))]\n",
    "test_data = dataset[int(0.7*len(dataset)):]\n",
    "\n",
    "# Навчаємо класифікатор\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Оцінюємо точність\n",
    "print(\"Accuracy:\", classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667f38e-1c07-44d1-ade7-7603ee6b8039",
   "metadata": {},
   "source": [
    "Створюємо функцію для прогнозування:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0219dc2a-70da-426a-b16d-febaecdcb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Netflix earnings are great!': Positive\n",
      "'I hate this bank service': Negative\n",
      "'Market is crashing': Negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(text):\n",
    "    tokens = process_text(text)\n",
    "    tokens_dict = dict([token, True] for token in tokens)\n",
    "    return classifier.classify(tokens_dict)\n",
    "\n",
    "# Тестуємо\n",
    "test_phrases = [\n",
    "    \"Netflix earnings are great!\",\n",
    "    \"I hate this bank service\",\n",
    "    \"Market is crashing\"\n",
    "]\n",
    "\n",
    "for phrase in test_phrases:\n",
    "    print(f\"'{phrase}': {predict_sentiment(phrase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50568b9-5f8f-46c1-ae62-7c8eb6845d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21677349-565d-4dd8-8829-25a6feed8149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328730ee-66c0-4966-af0f-8af786deae34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c89dee-04c8-4ea9-bbed-00362a80c551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "773d74ad-eb0d-444d-9ba4-9619dfae7d19",
   "metadata": {},
   "source": [
    "Task 2. Try to use Logistic Regression classifier instead and compare the results with Naive Bayes.\n",
    "\n",
    "Завдання 2. Спробуйте використати класифікатор логістичної регресії та порівняйте результати з наївним байєсівським класифікатором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5fde4711-2397-41fe-a65f-65b005478e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Accuracy: 0.696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Готуємо дані для sklearn\n",
    "all_texts = [\" \".join(process_text(tweet)) for tweet in data['tweet']]\n",
    "labels = data['label'].apply(lambda x: \"Positive\" if x == 1 else \"Negative\" if x == -1 else \"Neutral\")\n",
    "\n",
    "# Векторизуємо\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Розділяємо дані\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Навчаємо модель\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозуємо\n",
    "predictions = lr_model.predict(X_test)\n",
    "print(\"LogReg Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69cb07-2570-4de1-a229-948dfd6741ea",
   "metadata": {},
   "source": [
    "Task 3*. Suggest improvements to the code. How can the code be made faster/more accurate?\n",
    "\n",
    "Завдання 3*. Запропонуйте покращення до коду. Як можна зробити код швидшим або точнішим?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b45861-0306-4f79-b5cb-20d09bc91f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
