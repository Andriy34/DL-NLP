{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cd36fa-6a58-4290-bbfb-e63834442ef2",
   "metadata": {},
   "source": [
    "## Lab6 PyTorch: Модулі та шари\n",
    "\n",
    "### Що таке модулі?\n",
    "\n",
    "Модуль — це проміжний рівень абстракції між одним шаром нейронної мережі та повною моделлю.\n",
    "У PyTorch модулі — це основні будівельні блоки, з яких складаються як окремі шари, так і цілі мережі.\n",
    "Вони дозволяють гнучко поєднувати, вкладати та повторно використовувати різні компоненти мережі.\n",
    "\n",
    "---\n",
    "\n",
    "Якщо потрібно, можу допомогти розписати далі або пояснити конкретні приклади використання модулів у PyTorch.\n",
    "\n",
    "\n",
    "![Multiple layers are combined into modules, forming repeating patterns of larger models.](./img/blocks.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955d842-31f5-4165-99c6-1b0e7b57b74a",
   "metadata": {},
   "source": [
    "З програмної точки зору, модуль у PyTorch — це клас. Будь-який його підклас має:\n",
    "\n",
    "* визначати метод **forward**, який описує, як вхідні дані перетворюються у вихідні (пряме поширення);\n",
    "* зберігати всі необхідні параметри (ваги, біаси тощо).\n",
    "\n",
    "Тобто модуль — це об’єкт із власним поведінкою (forward) і станом (параметрами), який можна використовувати як будівельний блок для нейронної мережі.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c79362-d815-476e-b74d-ec4807b4ec53",
   "metadata": {},
   "source": [
    "Start with necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009f256d-2d7f-4792-8dce-0207fea30075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094c0d0-18eb-439b-8186-5a28c349ddde",
   "metadata": {},
   "source": [
    "### Зв’язування модулів\n",
    "\n",
    "Ми можемо використовувати вбудовану функцію `Sequential`, щоб з’єднувати шари разом. Нижченаведений код створює прихований шар із 256 нейронами та функцією активації ReLU, а також вихідний шар із 10 нейронами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b94a2cc-6661-4bb9-868a-73493fd075c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c38b8d-e353-476a-9c99-3cd4264209a6",
   "metadata": {},
   "source": [
    "### Пряме поширення (Forward propagation)\n",
    "\n",
    "**Важливо**: щоб використати модель, ми передаємо їй вхідні дані (`net(X)` у наведеному вище прикладі). Це виконує метод `forward` моделі разом із деякими фоновими операціями.\n",
    "\n",
    "`net(X)` — це насправді скорочення для `net.__call__(X)`. `LazyLinear` — це версія `Linear`, яка *визначає* розміри виходу автоматично.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef08a5c9-9085-4bc0-8d5e-4056ed3e54ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad4a09-414d-4307-b5f8-1d9bfa6440b6",
   "metadata": {},
   "source": [
    "### Створення модуля з нуля\n",
    "\n",
    "У наведеному нижче фрагменті ми створюємо модуль з нуля, що відповідає багатошаровому перцептрону (MLP) з одним прихованим шаром, який містить 256 нейронів, та вихідним шаром розмірності 10. Зверніть увагу, що клас `MLP` нижче наслідує клас, який представляє модуль (`nn.Module`). Ми значною мірою використовуватимемо методи батьківського класу, додаючи лише свій власний конструктор (метод `__init__` у Python) і метод прямого поширення (`forward`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e824413e-ed71-4ed2-912d-5eda53d1ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Викликаємо конструктор батьківського класу nn.Module для виконання\n",
    "        # необхідної ініціалізації\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Визначаємо пряме поширення моделі, тобто як отримати\n",
    "    # необхідний вихід моделі на основі вхідних даних X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d5d3-4d60-4331-9c80-289bf739274f",
   "metadata": {},
   "source": [
    "Сфокусуймось спочатку на методі прямого поширення (forward propagation). Зверни увагу, що він приймає `X` як вхід, обчислює приховане представлення з застосуванням активаційної функції та видає логіти (вихід до softmax або іншої функції).\n",
    "\n",
    "У цій реалізації MLP обидва шари (hidden і out) є змінними екземпляра. Чому це логічно? Уяви, що ми створюємо два екземпляри MLP — `net1` і `net2`, і навчаємо їх на різних даних. Очевидно, ми очікуємо, що вони зберігатимуть різні параметри і відповідно представлятимуть **дві різні моделі**, а не спільні шари чи ваги.\n",
    "\n",
    "Отже, використання змінних екземпляра гарантує, що кожна модель має **власні, незалежні параметри**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4a1e68-72bb-4d1c-9ff7-92ca972e7923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d81a0-2fa9-46e3-8091-cf78b350785b",
   "metadata": {},
   "source": [
    "**Отже**: модуль може абстрагувати як окремі шари, так і цілі моделі — або будь-що посередині."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf303a1-63ef-4c1d-bcdc-ebd8e3aa7306",
   "metadata": {},
   "source": [
    "### Написання власного `Sequential`\n",
    "\n",
    "Потрібно реалізувати два ключові методи:\n",
    "\n",
    "1. Метод для додавання модулів по одному до списку.\n",
    "2. Метод прямого поширення (forward propagation), який передає вхідні дані через ланцюг модулів у тому самому порядку, в якому вони були додані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36232efc-63b0-4d6a-a32d-30983e09f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71238b-f244-4ea8-8c4d-cca487f69fb1",
   "metadata": {},
   "source": [
    "Note `add_module` and `children` funcs.\n",
    "\n",
    "Now we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0984f53-1798-450b-97f6-9f053bfa7635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594c962-1843-44a5-bb95-d288e09f1c73",
   "metadata": {},
   "source": [
    "### Виконання коду у методі прямого поширення\n",
    "\n",
    "Клас `Sequential` спрощує побудову моделей, дозволяючи зібрати нову архітектуру без потреби визначати власний клас.\n",
    "Однак не всі архітектури є простими ланцюжками шарів.\n",
    "Коли потрібна більша гнучкість, краще визначати власні блоки (класи).\n",
    "\n",
    "Наприклад, нам може знадобитися використання елементів керування потоком (як-от `if`, `for`) у методі `forward`.\n",
    "Також може бути потреба у виконанні довільних математичних операцій, а не лише стандартних шарів нейромереж.\n",
    "\n",
    "До цього моменту всі операції в мережах працювали або над активаціями, або над параметрами мережі.\n",
    "Проте іноді може знадобитися використовувати фіксовані значення, які:\n",
    "\n",
    "* не є результатами попередніх шарів,\n",
    "* не є параметрами, які оновлюються під час тренування.\n",
    "\n",
    "Такі значення називаються **постійними параметрами** (*constant parameters*).\n",
    "\n",
    "Наприклад, припустимо, ми хочемо реалізувати шар, який обчислює таку функцію:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}, \\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}\n",
    "$$\n",
    "\n",
    "де:\n",
    "\n",
    "* $\\mathbf{x}$ — вхід,\n",
    "* $\\mathbf{w}$ — параметр (вага, що навчається),\n",
    "* $c$ — задана стала, яка **не оновлюється** під час оптимізації.\n",
    "\n",
    "Це можна реалізувати через клас `FixedHiddenMLP`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee35bc6-88e6-4c40-9127-4c66611d441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Випадкові вагові параметри, для яких не обчислюються градієнти,\n",
    "        # тому вони залишаються сталими під час тренування\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Повторне використання повнозв’язного шару. Це еквівалентно\n",
    "        # спільному використанню параметрів між двома повнозв’язними шарами\n",
    "        X = self.linear(X)\n",
    "        # Керування потоком (циклічна умова)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d7933-b614-4297-9f3f-38576e35d52a",
   "metadata": {},
   "source": [
    "Ось переклад цього тексту українською:\n",
    "\n",
    "У цій моделі ми реалізували прихований шар, ваги якого (`self.rand_weight`) ініціалізуються випадковим чином під час створення об’єкта і надалі залишаються сталими. Ця вага не є параметром моделі, тому вона ніколи не оновлюється за допомогою зворотного поширення помилки (backpropagation). Мережа передає вихід цього «фіксованого» шару через повнозв’язний шар.\n",
    "\n",
    "Зверніть увагу, що перед поверненням результату модель виконує незвичайну операцію. Виконується цикл `while`, який перевіряє, чи норма $\\ell_1$ (сума абсолютних значень) вектора виходу більша за 1, і поки ця умова виконується, вихідний вектор ділиться на 2. Нарешті, повертається сума елементів у `X`.\n",
    "\n",
    "На нашу думку, жодна стандартна нейронна мережа не виконує таку операцію. Ця конкретна операція, можливо, не буде корисною у реальних задачах. Нашою метою є лише показати, як можна інтегрувати довільний код у процес обчислень вашої нейронної мережі.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c7a145-4a67-4c0d-a1c9-7f7a10783ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2549, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718c62f-71a0-4178-b332-24baf27b8866",
   "metadata": {},
   "source": [
    "Ми можемо поєднувати різні способи складання модулів разом. У наступному прикладі ми вкладемо модулі дещо креативно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec0b3fcb-bd1d-47ff-98ee-58ee6d852b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2163, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed6645-5c20-431e-ad47-4f119fad59fa",
   "metadata": {},
   "source": [
    "### Параметри\n",
    "\n",
    "Після того, як ми вибрали архітектуру\n",
    "і налаштували гіперпараметри,\n",
    "ми переходимо до циклу навчання,\n",
    "де наша мета — знайти значення параметрів,\n",
    "які мінімізують функцію втрат.\n",
    "Після навчання нам знадобляться ці параметри,\n",
    "щоб робити прогнози в майбутньому.\n",
    "Також іноді ми хочемо\n",
    "витягти параметри,\n",
    "можливо, щоб повторно використати їх в іншому контексті,\n",
    "зберегти модель на диск, щоб\n",
    "запустити її в іншому програмному забезпеченні,\n",
    "або проаналізувати їх,\n",
    "щоб отримати наукове розуміння.\n",
    "\n",
    "Зазвичай ми можемо\n",
    "ігнорувати дрібні деталі\n",
    "оголошення та маніпуляції параметрами,\n",
    "покладаючись на фреймворки глибокого навчання,\n",
    "які роблять основну роботу.\n",
    "Проте, коли ми відійдемо від\n",
    "простих послідовних архітектур зі стандартними шарами,\n",
    "іноді доведеться заглибитися в\n",
    "оголошення та маніпуляції параметрами.\n",
    "У цьому розділі ми розглянемо:\n",
    "\n",
    "* Доступ до параметрів для налагодження, діагностики та візуалізації.\n",
    "* Спільне використання параметрів між різними компонентами моделі.\n",
    "\n",
    "Візьмемо, наприклад, MLP з одним прихованим шаром:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe8a590-2758-4ccf-a9bd-3308117b35a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d166a42-7117-470e-9324-fcde37d10ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1979, 0.2029, 0.1851, 0.5028],\n",
       "        [0.4797, 0.5983, 0.3032, 0.9344]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b626984-f02f-408d-9ff4-63cde5d51ed6",
   "metadata": {},
   "source": [
    "Як ми отримуємо доступ до параметрів кожного шару? За допомогою індексації."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7208f765-47b0-4db4-b062-35e09ab42b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.2815,  0.1976,  0.0715, -0.2519],\n",
       "                      [-0.2177,  0.2184,  0.0602, -0.0451],\n",
       "                      [-0.1265,  0.2998, -0.3104, -0.2772],\n",
       "                      [-0.1027, -0.2258,  0.1394, -0.1953],\n",
       "                      [-0.0600,  0.4202,  0.3387, -0.0350],\n",
       "                      [-0.4331, -0.4423, -0.4857, -0.4939],\n",
       "                      [ 0.0347,  0.0272, -0.4824, -0.1348],\n",
       "                      [-0.0579, -0.3795, -0.4692,  0.3741]])),\n",
       "             ('bias',\n",
       "              tensor([ 0.3753, -0.1778, -0.2986,  0.0924, -0.1007, -0.3006, -0.0192, -0.1997]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b76f87-fc49-4d6c-adab-e1229742cd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc73fb-d2bb-4790-bf88-0610ce02bd68",
   "metadata": {},
   "source": [
    "Зверніть увагу, що параметри (ваги/зміщення) є екземплярами класу `torch.nn.parameter.Parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce71956-0d70-4d66-b2aa-d646a57a88cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0635,  0.0794, -0.2659,  0.1833,  0.2203,  0.2912, -0.1707,  0.0504]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bbfaf-cf3b-4fdb-836d-89999526a02e",
   "metadata": {},
   "source": [
    "We can use `data` field in order to extract actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b5c519-9cbd-489b-9b5e-1058d1bbe075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0635,  0.0794, -0.2659,  0.1833,  0.2203,  0.2912, -0.1707,  0.0504]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a283ef3f-045b-4043-8154-d975e0150947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd1d3012-1556-4fe6-9c25-dba619b0062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a46d7b-ae83-485a-acd3-47865e5db528",
   "metadata": {},
   "source": [
    "We can also access gradients for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92748a77-d83c-4333-982e-88782db78f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7586896-42c5-433b-bcdc-fb2d78bb2965",
   "metadata": {},
   "source": [
    "We can also access parameters all at once via `nn.Module.named_parameters()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "280ff598-6c7a-4c0f-9991-253e6f62c551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb65aae-bfb7-475e-81e5-55029fe3a9b4",
   "metadata": {},
   "source": [
    "Ми також можемо пов’язувати або ділитися параметрами між різними шарами.\n",
    "\n",
    "У наведеному нижче прикладі ми створюємо повнозв’язний шар,\n",
    "а потім використовуємо його параметри спеціально,\n",
    "щоб задати параметри іншого шару.\n",
    "Тут нам потрібно спочатку виконати прямий прохід (forward propagation)\n",
    "`net(X)`, перш ніж отримувати доступ до параметрів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef26c0f2-a5a5-4757-afa6-5528ea52030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9c4a0-c91e-4ca5-847c-22a757bf6381",
   "metadata": {},
   "source": [
    "У цьому прикладі показано, що параметри другого і третього шару зв’язані (пов’язані) між собою.\n",
    "Вони не просто рівні — вони представлені одним і тим самим тензором.\n",
    "Отже, якщо ми змінюємо один з параметрів,\n",
    "інший також змінюється.\n",
    "\n",
    "Можна запитати:\n",
    "що відбувається з градієнтами, коли параметри пов’язані?\n",
    "Оскільки параметри моделі містять градієнти,\n",
    "градієнти другого і третього шару додаються разом\n",
    "під час зворотного поширення (backpropagation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f9c29-3e21-4fff-9d4b-82713730eb59",
   "metadata": {},
   "source": [
    "### Ініціалізація\n",
    "\n",
    "За замовчуванням PyTorch ініціалізує матриці вагів та зсувів (bias) рівномірно, вибираючи значення з діапазону, який обчислюється залежно від розмірності входу та виходу. Модуль `nn.init` у PyTorch надає різні готові методи ініціалізації.\n",
    "\n",
    "Почнемо зі створення прикладної нейронної мережі за допомогою `nn.Sequential`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be23bc30-95af-46fc-99b3-450030659984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35b2f8-8287-432e-a656-817e36062f2b",
   "metadata": {},
   "source": [
    "### Вбудовані ініціалізатори\n",
    "\n",
    "Цей код ініціалізує ваги як випадкові змінні за нормальним (гаусовим) розподілом, а зсуви (bias) будуть рівні 0. Метод `apply` рекурсивно застосовує задану функцію до кожного підмодуля:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d078f42-0fd6-482c-808f-afea9c995109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0048,  0.0183, -0.0045,  0.0047]), tensor(0.))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb362fac-dbda-4ab3-bae5-058d7599908e",
   "metadata": {},
   "source": [
    "Or we can use constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37a98316-d0ee-4596-95da-a25989dec4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804dd21-9c49-4195-bca1-71f779d00b02",
   "metadata": {},
   "source": [
    "We can use indexing to initialize parameters for each layer separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd6aa9b3-e087-48cb-baae-08c11dbe2fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6308,  0.3981,  0.0355,  0.2596])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "def init_42(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b55dd2-67e4-4551-abc2-aaeb83429bbc",
   "metadata": {},
   "source": [
    "Іноді потрібні методи ініціалізації\n",
    "відсутні у фреймворку глибинного навчання.\n",
    "У наведеному нижче прикладі ми визначаємо ініціалізатор\n",
    "для будь-якого параметра ваги $w$, використовуючи наступний незвичайний розподіл (тут $U$ позначає рівномірний розподіл):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\textrm{ with probability } \\frac{1}{4} \\\\\n",
    "            0    & \\textrm{ with probability } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\textrm{ with probability } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24cd78f1-0179-4cca-a203-18b9d987030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  8.5447, -8.3079,  9.4334],\n",
       "        [-0.0000, -6.4583, -0.0000, -5.4168]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in module.named_parameters()][0])\n",
    "        nn.init.uniform_(module.weight, -10, 10)\n",
    "        module.weight.data *= module.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eefa29-b8fd-4aeb-a259-8f87cac43d1f",
   "metadata": {},
   "source": [
    "And, of course, we can always set the parameters directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02fee178-4573-4422-aceb-863763988b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  9.5447, -7.3079, 10.4334])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3f9b2-0ec4-4555-827a-1506b7c37097",
   "metadata": {},
   "source": [
    "Or in model class constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7aeb3be9-5835-4870-8df1-2def58908c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                   requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769aa889-d479-4051-8795-a3ad95511b72",
   "metadata": {},
   "source": [
    "### Лінива ініціалізація\n",
    "\n",
    "PyTorch може відкласти ініціалізацію параметрів, чекаючи першого проходу даних через модель, щоб динамічно визначити розміри кожного шару.\n",
    "\n",
    "Згодом, при роботі з згортковими нейронними мережами, цей підхід стане ще зручнішим, оскільки розмірність вхідних даних (наприклад, роздільна здатність зображення) впливає на розмірність кожного наступного шару.\n",
    "\n",
    "Давайте знову створимо персептрон:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea1b347b-a7e5-4b6f-af39-9d522904f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ab599-6d9e-458e-9572-0c49362e606f",
   "metadata": {},
   "source": [
    "Check first layer params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a448bcc7-5759-433d-a365-cb209934583e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<UninitializedParameter>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5d88e-fad4-42e7-8a4e-c89d0b736e62",
   "metadata": {},
   "source": [
    "We can pass some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d42255e9-35f3-4e51-95d0-42a53d1c10f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 20])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "\n",
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6da710-892e-470b-88fc-42e96f5a8a3a",
   "metadata": {},
   "source": [
    "### Custom layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931d155-3ead-4870-afe7-4c34e566ca43",
   "metadata": {},
   "source": [
    "Let's first try a layer without parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54fb24d6-05dc-4d9b-884f-80cd6988f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c74142-9ebe-42b9-973a-1f3826f32987",
   "metadata": {},
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95c96fe2-d4b3-47d3-aff1-25dbb35df111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890d991-407c-4e69-b58f-2487606330ff",
   "metadata": {},
   "source": [
    "We can use it in some models now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b1a5704-127c-4cc5-ae4c-c6801c7fc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e5a25-30ce-41d8-b099-49758ddc829c",
   "metadata": {},
   "source": [
    "Check how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "318398b4-6dec-4b34-addc-cb3d662e032d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5879e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b29d8-e64d-465d-9dcc-2203d562b5da",
   "metadata": {},
   "source": [
    "The very small number instead of 0 is due to floating point number arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5441bd-1d00-433c-a752-86a239d9decb",
   "metadata": {},
   "source": [
    "Let's now create a custom layer with parameters: `in_units` and `out_units` for inputs and outputs counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2955e5f8-ceac-4371-b204-33d9047005de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, out_units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, out_units))\n",
    "        self.bias = nn.Parameter(torch.randn(out_units,))\n",
    "\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240c690-1be7-4939-a39a-49fd31057b76",
   "metadata": {},
   "source": [
    "We can instantiate it and check parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a2ae7a4-af2b-40b6-8673-901c68189221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.3170, -0.8199,  0.0033],\n",
       "        [-1.5403, -1.3372, -0.7153],\n",
       "        [-0.4825, -1.8492, -0.0956],\n",
       "        [-0.2137, -0.0974,  0.4601],\n",
       "        [ 1.2898, -1.1746, -0.5523]], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa9171-eb11-498e-b185-ef37191fc93d",
   "metadata": {},
   "source": [
    "Now invoke forward propagation using our custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00f22d25-47ab-47b7-bec7-290a985b2120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4625, 0.0000, 0.0109],\n",
       "        [2.1471, 0.0000, 0.1843]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa6e5a-f5a1-407b-b0c3-3f53277b2859",
   "metadata": {},
   "source": [
    "Now we can construct a model using the custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fa22724-744d-402e-b5fb-964c62863789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1307],\n",
       "        [3.3386]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2b8b0-5cca-4247-90fb-b4da40614cd5",
   "metadata": {},
   "source": [
    "### Loading and saving\n",
    "\n",
    "Save a single tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40756f8d-ceba-4f85-aa06-b34d54a8e3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x, 'torch.data')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1bdbde-7de6-4254-bd3e-edd59b332b66",
   "metadata": {},
   "source": [
    "Now read it back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceb30ed0-c269-4841-b38b-35f67dbc9ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('torch.data')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434bce76-513c-41cd-9bbd-0de789817f09",
   "metadata": {},
   "source": [
    "Same can be done with lists of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aac3fe5c-fea1-43a4-b89c-999b413b3425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'torch2.data')\n",
    "x2, y2 = torch.load('torch2.data')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8175d2-d2f3-4604-9e03-c7a9beb04ef8",
   "metadata": {},
   "source": [
    "Saving dictionaries of tensors also works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af7adfb6-5175-41d3-a5e4-27afb905a139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'torch_dict.data')\n",
    "mydict2 = torch.load('torch_dict.data')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c72a14-beb9-4f73-94da-c7427d5b5e5d",
   "metadata": {},
   "source": [
    "How does this work for complete models? Let's check it, using the example model for multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e7fe20f-c33d-4506-93d3-72104c73a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75347af6-8ffb-4020-b46f-b9a657267d47",
   "metadata": {},
   "source": [
    "We can save the parameters of that model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28e4f792-56f6-4d25-952a-9576322f37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce711cb-a97d-4f48-90b8-697761281a1a",
   "metadata": {},
   "source": [
    "In order to load the model, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69336b4e-99e7-438c-94f2-b4d7e03eb67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbfa7f5-830c-4258-b700-a73f4e5d62db",
   "metadata": {},
   "source": [
    "Let's verify that original model and cloned model produce same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cf82b70-e5e0-41bd-a2b3-0773953c2188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099bea2b-71f0-4800-9781-44c8e26c6dfb",
   "metadata": {},
   "source": [
    "## Крок за кроком з датасетом TorchVision\n",
    "\n",
    "### Завантаження даних\n",
    "\n",
    "Давайте завантажимо приклад датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4b5e687-2000-4ce8-bf0d-4f44f4dbc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9588f7-92ee-459a-bf00-aec8547ecb22",
   "metadata": {},
   "source": [
    "PyTorch пропонує спеціалізовані бібліотеки для різних доменів, такі як TorchText, TorchVision і TorchAudio, кожна з яких містить власні датасети. Для цього навчального матеріалу ми будемо використовувати датасет з TorchVision.\n",
    "\n",
    "Модуль `torchvision.datasets` містить об’єкти Dataset для багатьох реальних наборів даних у сфері комп’ютерного зору, таких як CIFAR, COCO (повний список тут). Ми використаємо датасет FashionMNIST. Кожен датасет TorchVision включає два параметри: `transform` і `target_transform`, які відповідно дозволяють змінювати зразки (вхідні дані) і мітки (лейбли)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83967f42-c8d9-41b3-88c6-49b872a0989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4ba6f-e072-4f65-93e3-deb0f888e3c7",
   "metadata": {},
   "source": [
    "Ми передаємо `Dataset` як аргумент у `DataLoader`. Це створює ітератор для нашого датасету та підтримує автоматичне формування батчів, вибірку, перемішування даних і багатопроцесове завантаження. Тут ми задаємо розмір батчу рівним 64, тобто кожен елемент ітератора `DataLoader` буде повертати батч з 64 прикладів та їхніх міток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "597c3fef-7209-4c6c-8bb6-557c63955fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861a472-71dd-4bbe-9b0c-d2df1905f1d8",
   "metadata": {},
   "source": [
    "Let's visualize some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3df3b417-8851-4ea9-836b-507f369d1a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a2043-c7b2-434a-891f-971bb22fe03d",
   "metadata": {},
   "source": [
    "### Трансформації\n",
    "\n",
    "Усі датасети TorchVision мають два параметри — `transform` для зміни (преобразування) ознак (features) та `target_transform` для зміни міток (labels). Ці параметри приймають callable-об’єкти з логікою трансформації. Модуль `torchvision.transforms` пропонує кілька часто використовуваних трансформацій \"з коробки\".\n",
    "\n",
    "Особливості датасету FashionMNIST представлені у форматі PIL Image, а мітки — у вигляді цілих чисел. Для тренування нам потрібні ознаки у вигляді нормалізованих тензорів, а мітки — у вигляді one-hot кодування. Для цього можна використати трансформації `ToTensor` та `Lambda`.\n",
    "\n",
    "`target_transform` визначає функцію, яка перетворює ціле число у one-hot закодований тензор. Спочатку створюється тензор нулів розміром 10 (кількість класів у нашому датасеті), після чого метод `scatter_` встановлює значення 1 у позиції, що відповідає мітці `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1fb1bea-10b1-4d85-8da1-faca418f2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05915445-9ffc-44f8-a14a-ce0d94bf9497",
   "metadata": {},
   "source": [
    "### Defining a network model\n",
    "\n",
    "Now that we have the test data, define a neural network with 2 hidden layers and ReLU activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab4240e5-8e19-48b1-ac42-1dbf6d7c745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecf728-4384-4e2a-988f-3f248be6a03c",
   "metadata": {},
   "source": [
    "Розберемо кроки:\n",
    "\n",
    "* Ініціалізуємо шар `nn.Flatten`, який перетворює кожне 2D-зображення розміром 28×28 у безперервний масив з 784 пікселів (розмірність мінібачу на вимірі 0 зберігається).\n",
    "* `nn.Linear` — це модуль, який застосовує лінійне перетворення до вхідних даних, використовуючи збережені ваги та зсуви (біаси).\n",
    "* Після лінійних перетворень застосовуються активації `nn.ReLU` для введення нелінійності.\n",
    "* `nn.Sequential` впорядковує ці модулі у послідовність."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d12a6b-0fc2-4c82-b810-5e35e7baddda",
   "metadata": {},
   "source": [
    "In order to train a model, we also need to specify a loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3647e12-fb37-41db-86f0-3b1be736ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762ff64-e440-4c79-8f85-b98d2c69da3c",
   "metadata": {},
   "source": [
    "### Цикл навчання\n",
    "\n",
    "Підсумовуючи, щоб навчити модель, нам потрібно:\n",
    "\n",
    "* Даталоадер (dataloader), який працює з певним датасетом\n",
    "* Сама модель (що наслідує від PyTorch `nn.Module`)\n",
    "* Функція втрат (loss function)\n",
    "* Оптимізатор\n",
    "\n",
    "Необхідні кроки:\n",
    "\n",
    "* Виконати пряме поширення (forward propagation)\n",
    "* Обчислити втрати (loss)\n",
    "* Обчислити зворотне поширення (backward propagation)\n",
    "* Виконати крок градієнтного спуску (gradient descent step)\n",
    "\n",
    "\n",
    "![A picture](./img/training_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ec9e6a9-13dc-4219-8e88-97ed4b7085be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # This just sets the model in train mode\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91cc09-dd22-49bd-aff6-033b0fdac101",
   "metadata": {},
   "source": [
    "We'll also need a test function in order to verify how our model works. Note that we use `torch.no_grad()` as we don't need the gradients when testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "055b54f0-d711-48b3-8e61-4dfbfff167d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad2377-11bf-4ad2-a381-9daee9d92436",
   "metadata": {},
   "source": [
    "![A picture](./img/testing_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8becd2-820e-44fe-a89d-993faf5fd7c8",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "313d8ead-58f6-4aad-81d5-079e1aff19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306712  [    0/60000]\n",
      "loss: 2.295703  [ 6400/60000]\n",
      "loss: 2.275131  [12800/60000]\n",
      "loss: 2.268782  [19200/60000]\n",
      "loss: 2.256227  [25600/60000]\n",
      "loss: 2.229398  [32000/60000]\n",
      "loss: 2.229410  [38400/60000]\n",
      "loss: 2.206944  [44800/60000]\n",
      "loss: 2.204351  [51200/60000]\n",
      "loss: 2.155592  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 2.163995 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.180823  [    0/60000]\n",
      "loss: 2.170381  [ 6400/60000]\n",
      "loss: 2.118865  [12800/60000]\n",
      "loss: 2.125417  [19200/60000]\n",
      "loss: 2.082753  [25600/60000]\n",
      "loss: 2.026567  [32000/60000]\n",
      "loss: 2.042306  [38400/60000]\n",
      "loss: 1.979805  [44800/60000]\n",
      "loss: 1.974397  [51200/60000]\n",
      "loss: 1.886912  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.904422 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.944552  [    0/60000]\n",
      "loss: 1.914572  [ 6400/60000]\n",
      "loss: 1.806947  [12800/60000]\n",
      "loss: 1.827990  [19200/60000]\n",
      "loss: 1.731864  [25600/60000]\n",
      "loss: 1.687020  [32000/60000]\n",
      "loss: 1.690530  [38400/60000]\n",
      "loss: 1.608369  [44800/60000]\n",
      "loss: 1.620530  [51200/60000]\n",
      "loss: 1.504560  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.536690 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.610160  [    0/60000]\n",
      "loss: 1.572309  [ 6400/60000]\n",
      "loss: 1.430171  [12800/60000]\n",
      "loss: 1.484112  [19200/60000]\n",
      "loss: 1.375816  [25600/60000]\n",
      "loss: 1.375060  [32000/60000]\n",
      "loss: 1.378494  [38400/60000]\n",
      "loss: 1.311075  [44800/60000]\n",
      "loss: 1.337851  [51200/60000]\n",
      "loss: 1.239121  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.268129 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.347795  [    0/60000]\n",
      "loss: 1.325581  [ 6400/60000]\n",
      "loss: 1.166748  [12800/60000]\n",
      "loss: 1.261470  [19200/60000]\n",
      "loss: 1.141769  [25600/60000]\n",
      "loss: 1.170638  [32000/60000]\n",
      "loss: 1.187401  [38400/60000]\n",
      "loss: 1.126067  [44800/60000]\n",
      "loss: 1.160257  [51200/60000]\n",
      "loss: 1.081592  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.101198 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93478094-c439-40f2-8bdb-f6ba8d19d04c",
   "metadata": {},
   "source": [
    "Save and load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8970de6-ed4a-4c0d-8280-de113c03743a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d58eb-fb28-4dac-aba9-61ba4d123bec",
   "metadata": {},
   "source": [
    "Let's use the model to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "718577ea-e1ed-4831-bc21-581f0dadfad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b2119-0ba9-49d6-84e0-b9b21395fa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
