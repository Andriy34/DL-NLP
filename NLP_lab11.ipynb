{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55304484-bae2-46ca-8656-e4e29ad8674c",
   "metadata": {},
   "source": [
    "### NLP: Lab 11 (CNNs for NLP)  Using CNNs for NLP\n",
    "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑–≥–æ—Ä—Ç–∫–æ–≤–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –ø—Ä–∏—Ä–æ–¥–Ω–æ—ó –º–æ–≤–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75361f58-c760-4fdb-a296-41ab06783436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download(\"all\")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76454019-7c01-406a-ad09-a987addd5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from fastparquet) (1.26.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from fastparquet) (2.10.0)\n",
      "Requirement already satisfied: fsspec in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from fastparquet) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "da6ea4d0-2b28-4b03-9123-ca979774ab45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deezer.com 10,406,168 Artist DB\\n\\nWe have sc...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üö® ATTENTION ALL USERS! üö®\\n\\nüÜò Are you looking ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm working on a stats project to test some of...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[Sorry, I cannot generate inappropriate or sp...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L@@k at these Unbelievable diet pills that can...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0   Deezer.com 10,406,168 Artist DB\\n\\nWe have sc...  not_spam\n",
       "1  üö® ATTENTION ALL USERS! üö®\\n\\nüÜò Are you looking ...      spam\n",
       "2  I'm working on a stats project to test some of...  not_spam\n",
       "3  [[Sorry, I cannot generate inappropriate or sp...      spam\n",
       "4  L@@k at these Unbelievable diet pills that can...      spam"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö\n",
    "\n",
    "test = \"test-00000-of-00001-fa9b3e8ade89a333.parquet\"\n",
    "\n",
    "parquet_test =  pd.read_parquet(test)\n",
    "\n",
    "parquet_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5a738358-b1a9-41fd-a37e-f99c7097ed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ 2725 —Ç–µ–∫—Å—Ç—ñ–≤ –∑ –º—ñ—Ç–∫–∞–º–∏.\n",
      "\n",
      "–ü—Ä–∏–∫–ª–∞–¥ –¥–∞–Ω–∏—Ö:\n",
      "–¢–µ–∫—Å—Ç 1:  Deezer.com 10,406,168 Artist DB\n",
      "\n",
      "We have scraped ... (–ú—ñ—Ç–∫–∞: not_spam)\n",
      "–¢–µ–∫—Å—Ç 2: üö® ATTENTION ALL USERS! üö®\n",
      "\n",
      "üÜò Are you looking for a ... (–ú—ñ—Ç–∫–∞: spam)\n",
      "–¢–µ–∫—Å—Ç 3: I'm working on a stats project to test some of the... (–ú—ñ—Ç–∫–∞: not_spam)\n"
     ]
    }
   ],
   "source": [
    "# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–∞—Å–∏–≤—ñ–≤ texts —ñ labels\n",
    "\n",
    "# –í–∏—Ç—è–≥—É—î–º–æ —Ç–µ–∫—Å—Ç–∏ —Ç–∞ –º—ñ—Ç–∫–∏ —É numpy –º–∞—Å–∏–≤–∏\n",
    "texts = parquet_test['text'].to_numpy()  # —è–∫—â–æ —Å—Ç–æ–≤–ø–µ—Ü—å –∑ —Ç–µ–∫—Å—Ç–∞–º–∏ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è 'text'\n",
    "labels = parquet_test['label'].to_numpy()  # —è–∫—â–æ —Å—Ç–æ–≤–ø–µ—Ü—å –∑ –º—ñ—Ç–∫–∞–º–∏ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è 'label'\n",
    "\n",
    "# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ\n",
    "assert len(texts) == len(labels), \"–ü–æ–º–∏–ª–∫–∞: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–µ–∫—Å—Ç—ñ–≤ —ñ –º—ñ—Ç–æ–∫ –Ω–µ —Å–ø—ñ–≤–ø–∞–¥–∞—î!\"\n",
    "print(f\"\\n–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç—ñ–≤ –∑ –º—ñ—Ç–∫–∞–º–∏.\")\n",
    "\n",
    "# –í–∏–≤—ñ–¥ –ø—Ä–∏–∫–ª–∞–¥—É –¥–∞–Ω–∏—Ö\n",
    "print(\"\\n–ü—Ä–∏–∫–ª–∞–¥ –¥–∞–Ω–∏—Ö:\")\n",
    "for i in range(3):\n",
    "    print(f\"–¢–µ–∫—Å—Ç {i+1}: {texts[i][:50]}... (–ú—ñ—Ç–∫–∞: {labels[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b75db-cdc2-47a9-8aa9-7fc993b5bb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5277f67f-74a7-4f6b-9f2d-e5225df5add2",
   "metadata": {},
   "source": [
    "### Sentence pre-processing\n",
    "–ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ —Ä–µ—á–µ–Ω—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5042e499-a548-4ea1-a46c-f11cf300d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(texts):\n",
    "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): List of text data\n",
    "\n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        max_len (int): Maximum sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    max_len = 0\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "\n",
    "    # Add <pad> and <unk> tokens to the vocabulary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "\n",
    "    # Building our vocab from the corpus starting from index 2\n",
    "    idx = 2\n",
    "    for sent in texts:\n",
    "        # Tokenize a sentence\n",
    "        # CODE_START\n",
    "        tokenized_sent = word_tokenize(sent.lower())  # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—è –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É\n",
    "        # CODE_END\n",
    "\n",
    "        # Add `tokenized_sent` to `tokenized_texts`\n",
    "        tokenized_texts.append(tokenized_sent)\n",
    "\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_sent:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Update `max_len`\n",
    "        max_len = max(max_len, len(tokenized_sent))\n",
    "\n",
    "    return tokenized_texts, word2idx, max_len\n",
    "\n",
    "def encode(tokenized_texts, word2idx, max_len):\n",
    "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
    "    their index in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will be the input to our CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "        # Pad sentences to max_len\n",
    "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
    "\n",
    "        # Encode tokens to input_ids\n",
    "        # CODE_START\n",
    "        input_id = [word2idx.get(token, 1) for token in tokenized_sent]  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 1 (<unk>) –¥–ª—è –Ω–µ–≤—ñ–¥–æ–º–∏—Ö —Å–ª—ñ–≤\n",
    "        # CODE_END\n",
    "        input_ids.append(input_id)\n",
    "\n",
    "    return np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ce749-ffe7-4462-9639-cc7f0a0a7945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a6cc179-40a0-4f29-8339-2560ce9d71aa",
   "metadata": {},
   "source": [
    "### Pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2861092d-655c-4dc6-8cba-9b5a05e210cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n",
      "Loading pretrained vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/m95y8vmd14s17wr96v8ppn_c0000gn/T/ipykernel_22695/725594635.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm_notebook(fin):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c814bd76bdf649e79d81b4cfaa11a21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10002 / 13847 pretrained vectors found.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def load_pretrained_vectors(word2idx, fname):\n",
    "    \"\"\"Load pretrained vectors and create embedding layers.\n",
    "\n",
    "    Args:\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        fname (str): Path to pretrained vector file\n",
    "\n",
    "    Returns:\n",
    "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
    "            the size of word2idx and d is embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading pretrained vectors...\")\n",
    "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "\n",
    "    # Initialize random embeddings\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
    "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
    "\n",
    "    # Load pretrained vectors\n",
    "    count = 0\n",
    "    for line in tqdm_notebook(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
    "\n",
    "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
    "input_ids = encode(tokenized_texts, word2idx, max_len)\n",
    "\n",
    "# Load pretrained vectors\n",
    "embeddings = load_pretrained_vectors(word2idx, \"crawl-300d-2M.vec\")\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395e666-9fcd-4fbf-a7f4-bdb3feca4a95",
   "metadata": {},
   "source": [
    "### PyTorch data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d1ef8a8-b040-4f29-866e-3620ba2c3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–î–∞–Ω—ñ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!\n",
      "–£—Å—å–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: 2725\n",
      "–ù–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ: 2588 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤\n",
      "–í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ: 137 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤\n",
      "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –±–∞—Ç—á—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è: 51 (—Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞: 50)\n",
      "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –±–∞—Ç—á—ñ–≤ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö\n",
    "test = \"test-00000-of-00001-fa9b3e8ade89a333.parquet\"\n",
    "parquet_test = pd.read_parquet(test)\n",
    "\n",
    "# 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö\n",
    "texts = parquet_test['text'].astype(str).tolist()  # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—å, —â–æ —Ç–µ–∫—Å—Ç - —Ä—è–¥–æ–∫\n",
    "labels = parquet_test['label'].values\n",
    "\n",
    "# 3. –ö–æ–¥—É–≤–∞–Ω–Ω—è –º—ñ—Ç–æ–∫\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# 4. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # –ê–±–æ —ñ–Ω—à–∞ –º–æ–¥–µ–ª—å\n",
    "tokenized_data = tokenizer(\n",
    "    texts,\n",
    "    padding='max_length',  # –î–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏\n",
    "    truncation=True,       # –û–±—Ä—ñ–∑–∞–Ω–Ω—è –¥–æ–≤–≥–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤\n",
    "    max_length=128,       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ\n",
    "    return_tensors='pt'   # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ PyTorch —Ç–µ–Ω–∑–æ—Ä–∏\n",
    ")\n",
    "\n",
    "# 5. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω–∞–≤—á–∞–ª—å–Ω–∏–π —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –Ω–∞–±–æ—Ä–∏\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(texts)),\n",
    "    test_size=0.05,        # 5% –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó\n",
    "    random_state=42,       # –î–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ\n",
    "    stratify=encoded_labels # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤\n",
    ")\n",
    "\n",
    "# 6. –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è DataLoader\n",
    "def create_dataloader(input_ids, attention_masks, labels, batch_size=50, is_train=True):\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(dataset) if is_train else SequentialSampler(dataset)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=is_train  # –Ü–≥–Ω–æ—Ä—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –Ω–µ–ø–æ–≤–Ω–∏–π –±–∞—Ç—á –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "    )\n",
    "\n",
    "# 7. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataLoader –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "train_dataloader = create_dataloader(\n",
    "    tokenized_data['input_ids'][train_idx],\n",
    "    tokenized_data['attention_mask'][train_idx],\n",
    "    torch.tensor(encoded_labels[train_idx]),\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "# 8. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataLoader –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó\n",
    "val_dataloader = create_dataloader(\n",
    "    tokenized_data['input_ids'][val_idx],\n",
    "    tokenized_data['attention_mask'][val_idx],\n",
    "    torch.tensor(encoded_labels[val_idx]),\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# –í–∏–≤–µ–¥–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "print(\"\\n–î–∞–Ω—ñ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!\")\n",
    "print(f\"–£—Å—å–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: {len(texts)}\")\n",
    "print(f\"–ù–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ: {len(train_idx)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤\")\n",
    "print(f\"–í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ: {len(val_idx)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤\")\n",
    "print(f\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –±–∞—Ç—á—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è: {len(train_dataloader)} (—Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞: {train_dataloader.batch_size})\")\n",
    "print(f\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –±–∞—Ç—á—ñ–≤ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ad6b9-c2b2-4370-be0f-0533b7564dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab2207-720e-4aa4-9257-86259e054914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7564df-d9a7-4d24-a070-8900efc42d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c8fc41d-d84d-4c8d-9381-5e5f960bca59",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a471c181-72dd-4db6-b75b-3d3c867d6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample configuration:\n",
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "960cb64e-bbd0-4b53-bfb9-f536a507aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_NLP(nn.Module):\n",
    "    \"\"\"1D –ó–≥–æ—Ä—Ç–∫–æ–≤–∞ –ú–µ—Ä–µ–∂–∞ –¥–ª—è –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –†–µ—á–µ–Ω—å.\"\"\"\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∫–ª–∞—Å—É CNN_NLP.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç–∏:\n",
    "            pretrained_embedding (torch.Tensor): –ü–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –µ–º–±–µ–¥—ñ–Ω–≥–∏\n",
    "                —Ä–æ–∑–º—ñ—Ä—É (vocab_size, embed_dim)\n",
    "            freeze_embedding (bool): –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å False –¥–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤.\n",
    "                –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: False\n",
    "            vocab_size (int): –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞, –ø–æ—Ç—Ä—ñ–±–µ–Ω —è–∫—â–æ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è\n",
    "                –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –µ–º–±–µ–¥—ñ–Ω–≥–∏.\n",
    "            embed_dim (int): –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤ —Å–ª—ñ–≤. –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 300\n",
    "            filter_sizes (List[int]): –°–ø–∏—Å–æ–∫ —Ä–æ–∑–º—ñ—Ä—ñ–≤ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤. –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: [3, 4, 5]\n",
    "            num_filters (List[int]): –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É.\n",
    "                –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: [100, 100, 100]\n",
    "            num_classes (int): –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤. –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 2\n",
    "            dropout (float): –†—ñ–≤–µ–Ω—å dropout. –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "        # –®–∞—Ä –µ–º–±–µ–¥—ñ–Ω–≥—É\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        \n",
    "        # –ó–≥–æ—Ä—Ç–∫–æ–≤—ñ —à–∞—Ä–∏\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        \n",
    "        # –ü–æ–≤–Ω–æ–∑–≤'—è–∑–Ω–∏–π —à–∞—Ä\n",
    "        self.fc = nn.Linear(sum(num_filters), num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"–ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥ —á–µ—Ä–µ–∑ –º–µ—Ä–µ–∂—É.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç–∏:\n",
    "            input_ids (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "                —Ä–æ–∑–º—ñ—Ä—É (batch_size, max_sent_length)\n",
    "\n",
    "        –ü–æ–≤–µ—Ä—Ç–∞—î:\n",
    "            logits (torch.Tensor): –í–∏—Ö—ñ–¥–Ω—ñ –ª–æ–≥—ñ—Ç–∏ —Ä–æ–∑–º—ñ—Ä—É (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # –û—Ç—Ä–∏–º—É—î–º–æ –µ–º–±–µ–¥—ñ–Ω–≥–∏ –∑ input_ids. –§–æ—Ä–º–∞ –≤–∏—Ö–æ–¥—É: (b, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–ª—è –∑–≥–æ—Ä—Ç–∫–æ–≤–æ–≥–æ —à–∞—Ä—É. –§–æ—Ä–º–∞: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ CNN —Ç–∞ ReLU. –§–æ—Ä–º–∞ –≤–∏—Ö–æ–¥—É: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # –ú–∞–∫—Å-–ø—É–ª—ñ–Ω–≥. –§–æ—Ä–º–∞ –≤–∏—Ö–æ–¥—É: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "\n",
    "        # –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –ø–æ–¥–∞—á—ñ —É –ø–æ–≤–Ω–æ–∑–≤'—è–∑–Ω–∏–π —à–∞—Ä\n",
    "        # –§–æ—Ä–º–∞ –≤–∏—Ö–æ–¥—É: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "\n",
    "        # –û–±—á–∏—Å–ª—é—î–º–æ –ª–æ–≥—ñ—Ç–∏. –§–æ—Ä–º–∞ –≤–∏—Ö–æ–¥—É: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8c5d26c-fc24-47bf-8911-a9be6d54e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π (cpu, cuda –∞–±–æ mps)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2788a3e-229f-418c-b360-6334ef3d84d1",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4f6b2f0-5b69-464d-833f-0b8bb51dbe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initialize_model(pretrained_embedding=None,\n",
    "                    freeze_embedding=False,\n",
    "                    vocab_size=None,\n",
    "                    embed_dim=300,\n",
    "                    filter_sizes=[3, 4, 5],\n",
    "                    num_filters=[100, 100, 100],\n",
    "                    num_classes=2,\n",
    "                    dropout=0.5,\n",
    "                    learning_rate=0.01):\n",
    "    \"\"\"–°—Ç–≤–æ—Ä—é—î –µ–∫–∑–µ–º–ø–ª—è—Ä CNN –º–æ–¥–µ–ª—ñ —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞.\"\"\"\n",
    "\n",
    "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes —Ç–∞ \\\n",
    "    num_filters –ø–æ–≤–∏–Ω–Ω—ñ –º–∞—Ç–∏ –æ–¥–Ω–∞–∫–æ–≤—É –¥–æ–≤–∂–∏–Ω—É.\"\n",
    "\n",
    "    # –°—Ç–≤–æ—Ä—é—î–º–æ –µ–∫–∑–µ–º–ø–ª—è—Ä CNN –º–æ–¥–µ–ª—ñ\n",
    "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
    "                        freeze_embedding=freeze_embedding,\n",
    "                        vocab_size=vocab_size,\n",
    "                        embed_dim=embed_dim,\n",
    "                        filter_sizes=filter_sizes,\n",
    "                        num_filters=num_filters,\n",
    "                        num_classes=2,\n",
    "                        dropout=0.5)\n",
    "\n",
    "    # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π (GPU/CPU)\n",
    "    cnn_model.to(device)\n",
    "\n",
    "    # –°—Ç–≤–æ—Ä—é—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä RMSprop\n",
    "    optimizer = optim.RMSprop(cnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99dc24-8778-4bdf-a0f6-95d6f7901f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c58de24-d450-4a7b-9d7f-d4e5d7718c48",
   "metadata": {},
   "source": [
    "### Training loop \n",
    "—Ü–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6a6a1363-6284-405d-8cc0-40fbf9381858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# –í–∫–∞–∑—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –≤—Ç—Ä–∞—Ç - –∫—Ä–æ—Å-–µ–Ω—Ç—Ä–æ–ø—ñ—è\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"–í—Å—Ç–∞–Ω–æ–≤–ª—é—î seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
    "    \"\"\"–ù–∞–≤—á–∞–Ω–Ω—è CNN –º–æ–¥–µ–ª—ñ.\"\"\"\n",
    "\n",
    "    # –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # –ü–æ—á–∞—Ç–æ–∫ —Ü–∏–∫–ª—É –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "    print(\"–ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è...\\n\")\n",
    "    print(f\"{'–ï–ø–æ—Ö–∞':^7} | {'–í—Ç—Ä–∞—Ç–∏':^12} | {'–í—Ç—Ä–∞—Ç–∏ (–≤–∞–ª)':^10} | {'–¢–æ—á–Ω—ñ—Å—Ç—å (–≤–∞–ª)':^9} | {'–ß–∞—Å':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               –ù–∞–≤—á–∞–Ω–Ω—è\n",
    "        # =======================================\n",
    "\n",
    "        # –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ —á–∞—Å —Ç–∞ –≤—Ç—Ä–∞—Ç–∏\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # –ü–µ—Ä–µ–≤–æ–¥–∏–º–æ –º–æ–¥–µ–ª—å —É —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞—á –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π (GPU/CPU)\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # –û–±–Ω—É–ª—è—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∫—Ä–æ–∫—É\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥ - –æ—Ç—Ä–∏–º—É—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            # –û–±—á–∏—Å–ª—é—î–º–æ –≤—Ç—Ä–∞—Ç–∏ —Ç–∞ –∞–∫—É–º—É–ª—é—î–º–æ —ó—Ö –∑–Ω–∞—á–µ–Ω–Ω—è\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # –ó–≤–æ—Ä–æ—Ç–Ω—ñ–π –ø—Ä–æ—Ö—ñ–¥ —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ –≤—Ç—Ä–∞—Ç–∏ –∑–∞ –µ–ø–æ—Ö—É\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # =======================================\n",
    "        #               –í–∞–ª—ñ–¥–∞—Ü—ñ—è\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # –ü—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏ –æ—Ü—ñ–Ω—é—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É —Ç–æ—á–Ω—ñ—Å—Ç—å\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "\n",
    "            # –í–∏–≤–æ–¥–∏–º–æ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"–ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ù–∞–π–∫—Ä–∞—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {best_accuracy:.2f}%.\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"–û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏.\"\"\"\n",
    "    \n",
    "    # –ü–µ—Ä–µ–≤–æ–¥–∏–º–æ –º–æ–¥–µ–ª—å —É —Ä–µ–∂–∏–º –æ—Ü—ñ–Ω–∫–∏ (–≤–∏–º–∏–∫–∞—î–º–æ dropout)\n",
    "    model.eval()\n",
    "\n",
    "    # –ó–º—ñ–Ω–Ω—ñ –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –±–∞—á—É –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É...\n",
    "    for batch in val_dataloader:\n",
    "        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞—á –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # –û–±—á–∏—Å–ª—é—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (–±–µ–∑ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤)\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # –û–±—á–∏—Å–ª—é—î–º–æ –≤—Ç—Ä–∞—Ç–∏\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (–∫–ª–∞—Å–∏ –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é)\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å (—É –≤—ñ–¥—Å–æ—Ç–∫–∞—Ö)\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # –°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç —Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç—ñ –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f0c83-00c9-4339-a343-e23beef18a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "228dd336-f6f5-47be-b8d0-24b6f00c216e",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "–û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ CNN –∑ —Ä—ñ–∑–Ω–∏–º–∏ –ø—ñ–¥—Ö–æ–¥–∞–º–∏ –¥–æ –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6b235fb8-955b-46d6-8f78-b5e5f6f2d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è...\n",
      "\n",
      " –ï–ø–æ—Ö–∞  |    –í—Ç—Ä–∞—Ç–∏    | –í—Ç—Ä–∞—Ç–∏ (–≤–∞–ª) | –¢–æ—á–Ω—ñ—Å—Ç—å (–≤–∞–ª) |    –ß–∞—Å   \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m set_seed(\u001b[32m42\u001b[39m)     \u001b[38;5;66;03m# –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ\u001b[39;00m\n\u001b[32m      3\u001b[39m cnn_rand, optimizer = initialize_model(vocab_size=\u001b[38;5;28mlen\u001b[39m(word2idx), \u001b[38;5;66;03m# –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞\u001b[39;00m\n\u001b[32m      4\u001b[39m                                       embed_dim=\u001b[32m300\u001b[39m,             \u001b[38;5;66;03m# –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤\u001b[39;00m\n\u001b[32m      5\u001b[39m                                       learning_rate=\u001b[32m0.25\u001b[39m,        \u001b[38;5;66;03m# –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è\u001b[39;00m\n\u001b[32m      6\u001b[39m                                       dropout=\u001b[32m0.5\u001b[39m)               \u001b[38;5;66;03m# –†—ñ–≤–µ–Ω—å dropout\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_rand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, train_dataloader, val_dataloader, epochs)\u001b[39m\n\u001b[32m     35\u001b[39m model.train()\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞—á –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π (GPU/CPU)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     b_input_ids, b_labels = \u001b[38;5;28mtuple\u001b[39m(t.to(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# –û–±–Ω—É–ª—è—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∫—Ä–æ–∫—É\u001b[39;00m\n\u001b[32m     42\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# CNN-rand: –í–µ–∫—Ç–æ—Ä–∏ —Å–ª—ñ–≤ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—é—Ç—å—Å—è –≤–∏–ø–∞–¥–∫–æ–≤–æ\n",
    "set_seed(42)     # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ\n",
    "cnn_rand, optimizer = initialize_model(vocab_size=len(word2idx), # –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞\n",
    "                                      embed_dim=300,             # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤\n",
    "                                      learning_rate=0.25,        # –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "                                      dropout=0.5)               # –†—ñ–≤–µ–Ω—å dropout\n",
    "\n",
    "train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29949e96-88c8-429e-a2e6-7b34d09e8f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m cnn_rand, optimizer = initialize_model(\n\u001b[32m      4\u001b[39m     vocab_size=\u001b[38;5;28mlen\u001b[39m(word2idx),  \u001b[38;5;66;03m# –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞\u001b[39;00m\n\u001b[32m      5\u001b[39m     embed_dim=\u001b[32m300\u001b[39m,             \u001b[38;5;66;03m# –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤\u001b[39;00m\n\u001b[32m      6\u001b[39m     learning_rate=\u001b[32m0.25\u001b[39m,        \u001b[38;5;66;03m# –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è\u001b[39;00m\n\u001b[32m      7\u001b[39m     dropout=\u001b[32m0.5\u001b[39m               \u001b[38;5;66;03m# –†—ñ–≤–µ–Ω—å dropout\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ 3 –µ–ø–æ—Ö–∏\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train(cnn_rand, optimizer, \u001b[43mtrain_dataloader\u001b[49m, val_dataloader, epochs=\u001b[32m3\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# CNN-static: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ fastText (–±–µ–∑ –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è)\u001b[39;00m\n\u001b[32m     13\u001b[39m set_seed(\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# CNN-rand: –í–µ–∫—Ç–æ—Ä–∏ —Å–ª—ñ–≤ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—é—Ç—å—Å—è –≤–∏–ø–∞–¥–∫–æ–≤–æ\n",
    "set_seed(42)  # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ\n",
    "cnn_rand, optimizer = initialize_model(\n",
    "    vocab_size=len(word2idx),  # –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞\n",
    "    embed_dim=300,             # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤\n",
    "    learning_rate=0.25,        # –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è\n",
    "    dropout=0.5               # –†—ñ–≤–µ–Ω—å dropout\n",
    ")\n",
    "# –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ 3 –µ–ø–æ—Ö–∏\n",
    "train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=3)\n",
    "\n",
    "# CNN-static: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ fastText (–±–µ–∑ –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è)\n",
    "set_seed(42)\n",
    "cnn_static, optimizer = initialize_model(\n",
    "    pretrained_embedding=embeddings,  # –ü–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –µ–º–±–µ–¥—ñ–Ω–≥–∏\n",
    "    freeze_embedding=True,            # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –µ–º–±–µ–¥—ñ–Ω–≥–∏ (–Ω–µ –Ω–∞–≤—á–∞—î–º–æ)\n",
    "    learning_rate=0.25,\n",
    "    dropout=0.5\n",
    ")\n",
    "train(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=3)\n",
    "\n",
    "# CNN-non-static: –ü–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ fastText –∑ –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è–º\n",
    "set_seed(42)\n",
    "cnn_non_static, optimizer = initialize_model(\n",
    "    pretrained_embedding=embeddings,\n",
    "    freeze_embedding=False,  # –î–æ–Ω–∞–≤—á–∞—î–º–æ –µ–º–±–µ–¥—ñ–Ω–≥–∏\n",
    "    learning_rate=0.25,\n",
    "    dropout=0.5\n",
    ")\n",
    "train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412df16-bf18-4062-8f9d-4d34618d1bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d76ecc-ec76-4cf3-bac8-1d553c0c3323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a1c6f84-c969-4d8c-9c99-0f4525f1f7dc",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c0e11d4-346b-40cd-883d-9dd3bfec4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model=cnn_rand.to(\"cpu\"), max_len=62):\n",
    "    \"\"\"Predict probability that a review is positive.\"\"\"\n",
    "\n",
    "    # Tokenize, pad and encode text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
    "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
    "\n",
    "    # Compute logits\n",
    "    logits = model.forward(input_id)\n",
    "\n",
    "    #  Compute probability\n",
    "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
    "\n",
    "    print(f\"This entry is {probs[1] * 100:.2f}% not spam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c26e3-ff9e-429e-98ec-b170a547495d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
