{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4QigLQO_jCg"
   },
   "source": [
    "# Lab9.1 The Annotated Encoder-Decoder with Attention\n",
    "### Пояснена модель Енкодер-Декодер з Механізмом Уваги (Attention)\n",
    "\n",
    "Нещодавно Александр Раш опублікував блогпост під назвою [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html), у якому він описує модель Transformer з роботи [Attention is All You Need](https://arxiv.org/abs/1706.03762). Цей допис можна розглядати як **приквел** до того: *ми реалізуємо кодувальник-декодувальник з механізмом уваги* на основі (керованих) рекурентних нейронних мереж, тісно дотримуючись оригінальної роботи про нейронний машинний переклад з увагою — [\"Neural Machine Translation by Jointly Learning to Align and Translate\"](https://arxiv.org/abs/1409.0473) авторства Бахданау та ін. (2015).\n",
    "\n",
    "Ідея полягає в тому, що ознайомлення з обома блогпостами допоможе вам краще зрозуміти дві дуже впливові архітектури типу \"послідовність у послідовність\". Якщо у вас є коментарі або пропозиції, будь ласка, дайте знати: [@BastingsJasmijn](https://twitter.com/BastingsJasmijn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OtwiZx2_jCi"
   },
   "source": [
    "# Архітектура Моделі\n",
    "\n",
    "Ми будемо моделювати ймовірність \\$p(Y\\mid X)\\$ цільової послідовності \\$Y=(y\\_1, \\dots, y\\_{N})\\$ за заданої вхідної (вихідної) послідовності \\$X=(x\\_1, \\dots, x\\_M)\\$ безпосередньо за допомогою нейронної мережі: **Кодувальника-Декодувальника**.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/bastings/annotated_encoder_decoder/blob/master/images/bahdanau.png?raw=1\" width=\"636\">\n",
    "\n",
    "#### Кодувальник\n",
    "\n",
    "Кодувальник зчитує вхідне речення (*внизу на схемі*) та створює послідовність прихованих станів \\$\\mathbf{h}\\_1, \\dots, \\mathbf{h}\\_M\\$, по одному для кожного слова вхідної послідовності. Ці стани мають відображати значення слова в контексті заданого речення.\n",
    "\n",
    "Ми використаємо двонаправлену рекурентну нейронну мережу (Bi-RNN) як кодувальник; зокрема, двонаправлену GRU (Bi-GRU).\n",
    "\n",
    "Перш за все ми **вбудовуємо** слова вхідної послідовності.\n",
    "Це означає, що для кожного слова ми просто знаходимо його **векторне представлення** (вбудовування слова) у (випадково ініціалізованій) таблиці пошуку.\n",
    "Векторне представлення для слова \\$i\\$ у реченні ми позначимо як \\$\\mathbf{x}\\_i\\$.\n",
    "Завдяки вбудовуванню слів модель може використовувати той факт, що певні слова (наприклад, *cat* і *dog*) є семантично подібними і можуть оброблятися схожим чином.\n",
    "\n",
    "Як же отримати приховані стани \\$\\mathbf{h}\\_1, \\dots, \\mathbf{h}\\_M\\$?\n",
    "Одна GRU (передня) читає речення зліва направо, інша (задня) — справа наліво.\n",
    "Кожна з них працює за простою рекурсивною формулою:\n",
    "$\\mathbf{h}_j = \\text{GRU}( \\mathbf{x}_j , \\mathbf{h}_{j - 1} )$\n",
    "Тобто, ми отримуємо наступний стан на основі попереднього стану та поточного векторного представлення слова.\n",
    "\n",
    "Прихований стан передньої GRU на кроці \\$j\\$ міститиме інформацію про те, які слова **передують** слову на цій позиції, але не знатиме, що буде далі. Навпаки, задня GRU знатиме лише про ті слова, які **слідують за** словом на позиції \\$j\\$.\n",
    "Об’єднуючи ці два стани (**конкатенація**, позначено синім на схемі), ми отримуємо \\$\\mathbf{h}\\_j\\$, який представляє слово \\$j\\$ в контексті всього речення.\n",
    "\n",
    "\n",
    "\n",
    "#### Декодувальник\n",
    "\n",
    "Декодувальник (*у верхній частині схеми*) — це GRU з прихованим станом \\$\\mathbf{s}\\_i\\$. Він працює за подібною формулою до кодувальника, але приймає **додаткове входження** — вектор контексту \\$\\mathbf{c}\\_i\\$ (*показано жовтим кольором*).\n",
    "\n",
    "$\\mathbf{s}_{i} = f( \\mathbf{s}_{i - 1}, \\mathbf{y}_{i - 1}, \\mathbf{c}_i )$\n",
    "\n",
    "Тут \\$\\mathbf{y}\\_{i - 1}\\$ — це попереднє згенероване слово цільової послідовності (*не показано на схемі*).\n",
    "\n",
    "На кожному кроці механізм **уваги** динамічно визначає ту частину вхідного речення, яка є найбільш релевантною для передбачення поточного слова.\n",
    "Це досягається шляхом порівняння останнього стану декодувальника з кожним прихованим станом кодувальника.\n",
    "У результаті утворюється вектор контексту \\$\\mathbf{c}\\_i\\$ (*показано жовтим*).\n",
    "Механізм уваги буде детально пояснено пізніше.\n",
    "\n",
    "Після обчислення стану декодувальника \\$\\mathbf{s}\\_i\\$, нелінійна функція \\$g\\$ (яка застосовує [softmax](https://en.wikipedia.org/wiki/Softmax_function)) дає нам ймовірність поточного слова \\$y\\_i\\$:\n",
    "\n",
    "$p(y_i \\mid y_{<i}, x_1^M) = g(\\mathbf{s}_i, \\mathbf{c}_i, \\mathbf{y}_{i - 1})$\n",
    "\n",
    "Оскільки \\$g\\$ використовує softmax, результат — це вектор розміру словника вихідної мови, сума елементів якого дорівнює 1.0: це розподіл імовірностей по всіх словах.\n",
    "Під час тестування ми обираємо слово з найбільшою ймовірністю як поточний вихід перекладу.\n",
    "\n",
    "Для оптимізації використовується функція [втрат крос-ентропії (cross-entropy loss)](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy), щоб максимізувати ймовірність правильного вибору слова на кожному кроці.\n",
    "Усі параметри моделі (включаючи вбудовування слів) оновлюються для максимізації цієї ймовірності.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZgxUe5Q_jCi"
   },
   "source": [
    "# Попередні вимоги\n",
    "\n",
    "Цей підручник вимагає **PyTorch версії ≥ 0.4.1** і був протестований з **Python 3.6**.\n",
    "\n",
    "Переконайтеся, що у вас встановлені ці версії, і встановіть наведені нижче пакети, якщо вони ще не встановлені."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "j7tGtYEy_jCi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (3.10.3)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: regex in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from sacrebleu) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 lxml-5.4.0 portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy matplotlib sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0yMxLte__jCj",
    "outputId": "c0ede4a4-4e2c-4df2-e412-e5801077be33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# Ми будемо використовувати CUDA, якщо вона доступна\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzfPPWEJ_jCj"
   },
   "source": [
    "# Почнемо кодувати!\n",
    "\n",
    "## Клас моделі\n",
    "\n",
    "Наша базова модель `EncoderDecoder` дуже схожа на ту, що використовується у *The Annotated Transformer*.\n",
    "\n",
    "Одна з відмінностей полягає в тому, що наш кодувальник також повертає свої фінальні стани (`encoder_final` нижче), які використовуються для ініціалізації RNN-декодувальника.\n",
    "Також ми передаємо довжини послідовностей, оскільки RNN потребують цю інформацію.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W7F5iIiD_jCk"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "\n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "\n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZXGsjHV_jCk"
   },
   "source": [
    "Щоб усе залишалося простим, ми також залишаємо клас `Generator` без змін.\n",
    "\n",
    "Він просто проєктує попередній вихід (\\$x\\$ у функції `forward`, наведеної нижче) на вихідний шар, щоб отримати вектор, розмір якого відповідає розміру словника цільової мови.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wkq53_YF_jCk"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Br8oFVT_jCk"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Наш кодувальник — це двонаправлений GRU.\n",
    "\n",
    "Оскільки ми хочемо обробляти декілька речень одночасно для підвищення швидкості (це ефективніше на GPU), нам потрібно підтримувати **мікропакети (mini-batches)**.\n",
    "Речення в одному мікропакеті можуть мати різну довжину, а це означає, що RNN має розгортатися довше для деяких речень, тоді як для інших він може вже завершити обробку:\n",
    "\n",
    "```\n",
    "Приклад: мікропакет з 3 вхідних речень різної довжини (7, 5 та 3).\n",
    "Кінець послідовності позначено цифрою «3», а позиції доповнення (padding) — цифрою «1».\n",
    "\n",
    "+---------------+\n",
    "| 4 5 9 8 7 8 3 |\n",
    "+---------------+\n",
    "| 5 4 8 7 3 1 1 |\n",
    "+---------------+\n",
    "| 5 8 3 1 1 1 1 |\n",
    "+---------------+\n",
    "```\n",
    "Ви можете помітити, що при обчисленні прихованих станів для цього мікропакету для речень №2 та №3 нам потрібно припинити оновлення прихованого стану після того, як ми зустріли символ \"3\" (кінець послідовності). Ми не хочемо враховувати значення доповнення (1-ці).\n",
    "\n",
    "На щастя, у PyTorch є зручні допоміжні функції `pack_padded_sequence` і `pad_packed_sequence`.\n",
    "Вони автоматично обробляють маскування та доповнення, так що після завершення речення відповідні векторні представлення просто заповнюються нулями.\n",
    "\n",
    "Код нижче зчитує вхідне речення (послідовність вбудованих векторів слів) і генерує приховані стани.\n",
    "Також він повертає фінальний вектор — узагальнення усього речення, яке отримується конкатенацією першого і останнього прихованих станів (вони обидва «побачили» все речення, але у різних напрямках). Цей фінальний вектор ми використаємо для ініціалізації декодувальника.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VDBOyL7y_jCk"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # Нам потрібно вручну об’єднати (конкатенувати) фінальні стани обох напрямків.\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
    "\n",
    "        return output, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj0lQnQg_jCk"
   },
   "source": [
    "### Декодувальник\n",
    "\n",
    "Декодувальник — це умовний GRU.\n",
    "На відміну від кодувальника, який починає з порожнього стану, початковий прихований стан декодувальника отримується шляхом проєкції фінального вектора кодувальника.\n",
    "\n",
    "#### Навчання\n",
    "\n",
    "У методі `forward` є цикл, який обчислює приховані стани декодувальника по одному кроку за раз.\n",
    "Варто зауважити, що під час навчання ми точно знаємо, якими мають бути цільові слова! (Вони знаходяться в `trg_embed`.) Це означає, що ми навіть не перевіряємо тут, що передбачає модель! Ми просто подаємо правильне попереднє вбудоване слово цільової послідовності на GRU на кожному кроці. Це називається **teacher forcing** (навчання з підказкою).\n",
    "\n",
    "Метод `forward` повертає всі приховані стани декодувальника та вектори перед виходом (pre-output). Ці дані потім використовуються для обчислення функції втрат, після чого оновлюються параметри моделі.\n",
    "\n",
    "#### Прогнозування\n",
    "\n",
    "Під час прогнозування метод `forward` виконується тільки для одного кроку.\n",
    "Після передбачення слова з поверненого вектора pre-output, ми можемо викликати `forward` знову, передаючи в нього вбудування раніше передбаченого слова та останній прихований стан.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hMqyD69w_jCl"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "\n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "\n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
    "        context, attn_probs = self.attention(\n",
    "            query=query, proj_key=proj_key,\n",
    "            value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "\n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final,\n",
    "                src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "\n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "\n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "\n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "\n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAlWDVMG_jCl"
   },
   "source": [
    "### Attention                                                                                                                                                                               \n",
    "\n",
    "На кожному кроці декодувальник має доступ до *всіх* векторних представлень слів джерела — \\$\\mathbf{h}\\_1, \\dots, \\mathbf{h}\\_M\\$.\n",
    "Механізм уваги дозволяє моделі зосередитися на найбільш релевантній частині вхідного речення на даний момент.\n",
    "Стан декодувальника описується прихованим станом GRU — \\$\\mathbf{s}\\_i\\$.\n",
    "Щоб визначити, які векторні представлення слів джерела \\$\\mathbf{h}\\_j\\$ є найбільш релевантними, потрібно задати функцію, що приймає ці два аргументи на вхід.\n",
    "\n",
    "Тут ми використовуємо багатошаровий перцептрон (MLP) для адитивної уваги, як це зроблено у роботі Бахданау та співавт. (Bahdanau et al.):\n",
    "\n",
    "<img src=\"https://github.com/bastings/annotated_encoder_decoder/blob/master/images/attention.png?raw=1\" width=\"280\">\n",
    "\n",
    "\n",
    "Ми застосовуємо багатошаровий перцептрон (MLP) з активацією tanh як до поточного стану декодувальника \\$\\mathbf{s}\\_i\\$ (це *запит*), так і до кожного стану кодувальника \\$\\mathbf{h}*j\\$ (це *ключ*), а потім проєктуємо результат у одне скалярне значення — *енергію уваги* \\$e*{ij}\\$.\n",
    "\n",
    "Після обчислення всіх енергій вони нормалізуються за допомогою softmax, щоб їх сума дорівнювала одиниці:\n",
    "\n",
    "$$ \\alpha_{ij} = \\text{softmax}(\\mathbf{e}_i)[j] $$\n",
    "\n",
    "$$\\sum_j \\alpha_{ij} = 1.0$$\n",
    "\n",
    "Вектор контексту на кроці часу \\$i\\$ — це зважена сума прихованих станів кодувальника (тобто *значень*):\n",
    "$$\\mathbf{c}_i = \\sum_j \\alpha_{ij} \\mathbf{h}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "38rs5N84_jCl"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "\n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "\n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "\n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas\n",
    "\n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas, value)\n",
    "\n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUCiV_PX_jCl"
   },
   "source": [
    "## Вбудовування (Embeddings) та Softmax\n",
    "\n",
    "Ми використовуємо навчальні вбудовування (learned embeddings), щоб перетворити вхідні та вихідні токени у вектори розмірності `emb_size`.\n",
    "\n",
    "Для цього ми просто використовуємо стандартні засоби PyTorch — `nn.Embedding`.\n",
    " [nn.Embedding](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iED5mTJt_jCl"
   },
   "source": [
    "## Повна модель\n",
    "\n",
    "Тут ми визначаємо функцію, яка за гіперпараметрами створює повну модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fIngFgYL_jCl"
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1cmOG1q_jCm"
   },
   "source": [
    "# Навчання\n",
    "\n",
    "У цьому розділі описується режим навчання наших моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fLcIOr9_jCm"
   },
   "source": [
    "Зробимо невелику паузу, щоб познайомитись із деякими інструментами, необхідними для навчання стандартної моделі кодувальник-декодувальник.\n",
    "Спочатку визначимо об’єкт `batch`, який зберігатиме вхідні (src) та цільові (target) речення для навчання, а також їх довжини і маски."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-aAl78Y_jCm"
   },
   "source": [
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vd-jETEv_jCm"
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "\n",
    "        src, src_lengths = src\n",
    "\n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "\n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "\n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkGoayQL_jCm"
   },
   "source": [
    "## Цикл навчання\n",
    "\n",
    "Код нижче навчає модель протягом 1 епохи (1 проходу по тренувальних даних)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3qOpWStG_jCm"
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "\n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "\n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-Gxi6Ix_jCm"
   },
   "source": [
    "## Навчальні дані та формування батчів\n",
    "\n",
    "Для формування батчів ми використовуватимемо бібліотеку torchtext. Про це буде розказано детальніше нижче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgV0c3XG_jCm"
   },
   "source": [
    "## Оптимізатор\n",
    "\n",
    "Ми будемо використовувати [оптимізатор Adam](https://arxiv.org/abs/1412.6980) з типовими налаштуваннями (\\$\\beta\\_1=0.9\\$, \\$\\beta\\_2=0.999\\$, \\$\\epsilon=10^{-8}\\$).\n",
    "\n",
    "Як швидкість навчання (learning rate) тут оберемо \\$0.0003\\$, але для інших задач може підійти інше значення. Потрібно буде підбирати це параметр експериментально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkkZWGrg_jCm"
   },
   "source": [
    "# Перший приклад\n",
    "\n",
    "Можемо почати з простої задачі копіювання. Маючи випадковий набір вхідних символів із невеликого словника, мета — згенерувати ті ж самі символи у вихідній послідовності."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g6Lv6Dq_jCm"
   },
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uQVEsVwO_jCm"
   },
   "outputs": [],
   "source": [
    "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
    "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
    "    for i in range(num_batches):\n",
    "        data = torch.from_numpy(\n",
    "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
    "        data[:, 0] = sos_index\n",
    "        data = data.cuda() if USE_CUDA else data\n",
    "        src = data[:, 1:]\n",
    "        trg = data\n",
    "        src_lengths = [length-1] * batch_size\n",
    "        trg_lengths = [length] * batch_size\n",
    "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3k2SFkR_jCn"
   },
   "source": [
    "## Обчислення функції втрат (Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SovpAg-n_jCn"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YksOwdjE_jCn"
   },
   "source": [
    "### Вивід прикладів\n",
    "\n",
    "Щоб відстежувати прогрес під час навчання, ми будемо перекладати кілька прикладів.\n",
    "\n",
    "Для простоти використовуємо жадібне декодування (greedy decoding): на кожному кроці, починаючи з першого токена, вибираємо слово з максимальною ймовірністю і не повертаємось до попередніх виборів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qeyKGpJj_jCn"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "\n",
    "    output = np.array(output)\n",
    "\n",
    "    # cut off everything starting from </s>\n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]\n",
    "\n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "\n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hlOMEKfJ_jCn"
   },
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100,\n",
    "                   sos_index=1,\n",
    "                   src_eos_index=None,\n",
    "                   trg_eos_index=None,\n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "\n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "\n",
    "    for i, batch in enumerate(example_iter):\n",
    "\n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg\n",
    "\n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "\n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-V7P3f-_jCn"
   },
   "source": [
    "## Навчання задачі копіювання"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6nS1P6QJ_jCo"
   },
   "outputs": [],
   "source": [
    "def train_copy_task():\n",
    "    \"\"\"Train the simple copy task.\"\"\"\n",
    "    num_words = 11\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n",
    "    model = make_model(num_words, num_words, emb_size=32, hidden_size=64)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "    eval_data = list(data_gen(num_words=num_words, batch_size=1, num_batches=100))\n",
    "\n",
    "    dev_perplexities = []\n",
    "\n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        data = data_gen(num_words=num_words, batch_size=32, num_batches=100)\n",
    "        run_epoch(data, model,\n",
    "                  SimpleLossCompute(model.generator, criterion, optim))\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            perplexity = run_epoch(eval_data, model,\n",
    "                                   SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Evaluation perplexity: %f\" % perplexity)\n",
    "            dev_perplexities.append(perplexity)\n",
    "            print_examples(eval_data, model, n=2, max_len=9)\n",
    "\n",
    "    return dev_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NsFqbuPx_jCo",
    "outputId": "50d8f227-7680-459d-ff8a-647da431aaa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boykosvitlana/dl_venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 50 Loss: 19.712404 Tokens per Sec: 9042.240143\n",
      "Epoch Step: 100 Loss: 17.804193 Tokens per Sec: 10960.161853\n",
      "Evaluation perplexity: 7.159559\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  5 8 7 5 8 7 5 5 8\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 8 8 8 8 8 8 8\n",
      "\n",
      "Epoch 1\n",
      "Epoch Step: 50 Loss: 15.360044 Tokens per Sec: 12528.844814\n",
      "Epoch Step: 100 Loss: 11.845239 Tokens per Sec: 12009.364755\n",
      "Evaluation perplexity: 3.776112\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 7 5 3 5 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 8 2 5 8 3 2\n",
      "\n",
      "Epoch 2\n",
      "Epoch Step: 50 Loss: 10.008592 Tokens per Sec: 10933.551577\n",
      "Epoch Step: 100 Loss: 9.149925 Tokens per Sec: 14242.390878\n",
      "Evaluation perplexity: 2.607750\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 7 8 10\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 8 5 2 6 8 3\n",
      "\n",
      "Epoch 3\n",
      "Epoch Step: 50 Loss: 7.402652 Tokens per Sec: 13777.690344\n",
      "Epoch Step: 100 Loss: 6.568831 Tokens per Sec: 16754.239422\n",
      "Evaluation perplexity: 2.043324\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 8 7 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 6 3 2 5 8 8 6\n",
      "\n",
      "Epoch 4\n",
      "Epoch Step: 50 Loss: 5.743893 Tokens per Sec: 13896.770595\n",
      "Epoch Step: 100 Loss: 4.973376 Tokens per Sec: 12183.636465\n",
      "Evaluation perplexity: 1.767207\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 8 7 10\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 5\n",
      "Epoch Step: 50 Loss: 5.255085 Tokens per Sec: 13756.799788\n",
      "Epoch Step: 100 Loss: 4.183656 Tokens per Sec: 14069.655419\n",
      "Evaluation perplexity: 1.564880\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 10 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 6\n",
      "Epoch Step: 50 Loss: 4.013874 Tokens per Sec: 13474.430876\n",
      "Epoch Step: 100 Loss: 3.831588 Tokens per Sec: 16191.580043\n",
      "Evaluation perplexity: 1.485504\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 5 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 8 6\n",
      "\n",
      "Epoch 7\n",
      "Epoch Step: 50 Loss: 3.080931 Tokens per Sec: 15368.786273\n",
      "Epoch Step: 100 Loss: 2.744050 Tokens per Sec: 10733.348154\n",
      "Evaluation perplexity: 1.410001\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 8\n",
      "Epoch Step: 50 Loss: 2.370628 Tokens per Sec: 16522.756373\n",
      "Epoch Step: 100 Loss: 2.534335 Tokens per Sec: 14605.514008\n",
      "Evaluation perplexity: 1.292480\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 5 7 8\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 9\n",
      "Epoch Step: 50 Loss: 2.354441 Tokens per Sec: 12626.549883\n",
      "Epoch Step: 100 Loss: 2.199166 Tokens per Sec: 12599.835154\n",
      "Evaluation perplexity: 1.206034\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the copy task\n",
    "dev_perplexities = train_copy_task()\n",
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "\n",
    "plot_perplexity(dev_perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3KosG8w_jCo"
   },
   "source": [
    "Ви можете побачити, що модель змогла правильно «перекласти» два приклади наприкінці.\n",
    "\n",
    "Крім того, перплексія на валідаційних даних гарно знизилася до близько 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm3_IWbi_jCo"
   },
   "source": [
    "# Приклад з реального світу\n",
    "\n",
    "Тепер розглянемо реальний приклад на задачі перекладу з німецької на англійську з набору даних IWSLT.\n",
    "Ця задача набагато менша за звичайні, але добре ілюструє роботу всієї системи.\n",
    "\n",
    "Наступна комірка встановлює бібліотеки torchtext і spaCy. Це може зайняти деякий час.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "EpexGQqz_jCo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/pytorch/text\n",
      "  Cloning git://github.com/pytorch/text to /private/var/folders/d_/m95y8vmd14s17wr96v8ppn_c0000gn/T/pip-req-build-jwlyr3r1\n",
      "  Running command git clone --filter=blob:none --quiet git://github.com/pytorch/text /private/var/folders/d_/m95y8vmd14s17wr96v8ppn_c0000gn/T/pip-req-build-jwlyr3r1\n",
      "  fatal: unable to connect to github.com:\n",
      "  github.com[0: 140.82.121.3]: errno=Operation timed out\n",
      "\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet git:\u001b[0m\u001b[32m/\u001b[0m\u001b[32m/github.com/pytorch/\u001b[0m\u001b[32mtext\u001b[0m\u001b[32m \u001b[0m\u001b[32m/private/var/folders/d_/m95y8vmd14s17wr96v8ppn_c0000gn/T/\u001b[0m\u001b[32mpip-req-build-jwlyr3r1\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet git:\u001b[0m\u001b[32m/\u001b[0m\u001b[32m/github.com/pytorch/\u001b[0m\u001b[32mtext\u001b[0m\u001b[32m \u001b[0m\u001b[32m/private/var/folders/d_/m95y8vmd14s17wr96v8ppn_c0000gn/T/\u001b[0m\u001b[32mpip-req-build-jwlyr3r1\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/Users/boykosvitlana/dl_venv/bin/python: No module named spacy\n",
      "/Users/boykosvitlana/dl_venv/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install git+git://github.com/pytorch/text spacy\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzBw0vWf_jCp"
   },
   "source": [
    "## Завантаження даних\n",
    "\n",
    "Ми завантажимо датасет за допомогою torchtext і spacy для токенізації.\n",
    "\n",
    "Ця комірка може виконуватися довго при першому запуску, оскільки вона завантажує та токенізує дані IWSLT.\n",
    "\n",
    "Для прискорення ми включаємо лише короткі речення, а слово додаємо до словника тільки якщо воно зустрічається щонайменше 5 разів. У цьому випадку ми також переводимо дані у нижній регістр.\n",
    "\n",
    "Якщо у вас виникнуть **проблеми** з torchtext у наведеній комірці (наприклад, помилка `ascii`), спробуйте перед запуском `jupyter notebook` виконати команду:\n",
    "\n",
    "If you have **issues** with torch text in the cell below (e.g. an `ascii` error), try running `export LC_ALL=\"en_US.UTF-8\"` before you start `jupyter notebook`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ylbNrP2q_jCp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/boykosvitlana/dl_venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c105ErrorC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES7_PKv\n  Referenced from: /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in: /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages/torch/lib/libc10.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# For data loading.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data, datasets\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl_venv/lib/python3.11/site-packages/torchtext/__init__.py:18\u001b[39m\n\u001b[32m     15\u001b[39m     _WARN = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     20\u001b[39m _TEXT_BUCKET = \u001b[33m\"\u001b[39m\u001b[33mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m _CACHE_DIR = os.path.expanduser(os.path.join(_get_torch_home(), \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl_venv/lib/python3.11/site-packages/torchtext/_extension.py:64\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl_venv/lib/python3.11/site-packages/torchtext/_extension.py:58\u001b[39m, in \u001b[36m_init_extension\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils.is_module_available(\u001b[33m\"\u001b[39m\u001b[33mtorchtext._torchtext\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtorchtext C++ Extension is not found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlibtorchtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl_venv/lib/python3.11/site-packages/torchtext/_extension.py:50\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m(lib)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl_venv/lib/python3.11/site-packages/torch/_ops.py:1392\u001b[39m, in \u001b[36m_Ops.load_library\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1387\u001b[39m path = _utils_internal.resolve_library_path(path)\n\u001b[32m   1388\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[32m   1389\u001b[39m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[32m   1390\u001b[39m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[32m   1391\u001b[39m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28mself\u001b[39m.loaded_libraries.add(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ctypes/__init__.py:376\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: dlopen(/Users/boykosvitlana/dl_venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c105ErrorC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES7_PKv\n  Referenced from: /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in: /Users/boykosvitlana/dl_venv/lib/python3.11/site-packages/torch/lib/libc10.dylib"
     ]
    }
   ],
   "source": [
    "# For data loading.\n",
    "\n",
    "\n",
    "from torchtext import data, datasets\n",
    "import nltk\n",
    "\n",
    "if True:\n",
    "\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in nltk.tokenize(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in nltk.tokenize(text)]\n",
    "\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"\n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "\n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = data.Field(tokenize=tokenize_de,\n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    TRG = data.Field(tokenize=tokenize_en,\n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
    "        exts=('.de', '.en'), fields=(SRC, TRG),\n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and\n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGqwKrQU_jCp"
   },
   "source": [
    "### Давайте подивимось на дані\n",
    "\n",
    "Ніколи не зашкодить переглянути свої дані та деякі статистики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "f01hEt36_jCp",
    "outputId": "10ea26bb-635b-4261-b3e3-bdeffe603c72"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of German words (types):\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(src_field.vocab))\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of English words (types):\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trg_field.vocab), \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m print_data_info(\u001b[43mtrain_data\u001b[49m, valid_data, test_data, SRC, TRG)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "\n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
    "\n",
    "\n",
    "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjTLizhA_jCp"
   },
   "source": [
    "## Ітератори\n",
    "\n",
    "Пакетна обробка (batching) має величезне значення для швидкості. Тут ми використаємо `BucketIterator` з torchtext, щоб отримувати пакети речень, які мають (майже) однакову довжину.\n",
    "\n",
    "#### Примітка щодо сортування пакетів для RNN у PyTorch\n",
    "\n",
    "Для ефективності PyTorch RNN вимагають, щоб пакети були відсортовані за довжиною, причому найдовше речення в пакеті йде першим. Для тренування ми просто сортуємо кожен пакет.\n",
    "Для валідації можуть виникнути проблеми, якщо потрібно порівнювати наші переклади з якимось зовнішнім файлом, який не відсортований. Тому для валідації ми просто встановлюємо розмір пакету рівним 1, щоб зберегти оригінальний порядок.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCAMikUC_jCq"
   },
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=64, train=True,\n",
    "                                 sort_within_batch=True,\n",
    "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
    "                                 device=DEVICE)\n",
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False,\n",
    "                           device=DEVICE)\n",
    "\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQNp-1T3_jCq"
   },
   "source": [
    "## Training the System\n",
    "\n",
    "Now we train the model.\n",
    "\n",
    "On a Titan X GPU, this runs at ~18,000 tokens per second with a batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQNpEc0r_jCq"
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
    "    \"\"\"Train a model on IWSLT\"\"\"\n",
    "\n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    # optionally add label smoothing; see the Annotated Transformer\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    dev_perplexities = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"Epoch\", epoch)\n",
    "        model.train()\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter),\n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
    "                                     print_every=print_every)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter),\n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)\n",
    "\n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter),\n",
    "                                       model,\n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "\n",
    "    return dev_perplexities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6yYI-Pg_jCq",
    "outputId": "e0d1cead-4073-44ec-bd9a-72cde5aa4c97"
   },
   "outputs": [],
   "source": [
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=1, dropout=0.2)\n",
    "dev_perplexities = train(model, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3OKB6SB_jCq",
    "outputId": "0b92ea43-650a-4666-9a6a-ae6a0dd48c4b"
   },
   "outputs": [],
   "source": [
    "plot_perplexity(dev_perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORMR3UPq_jCq"
   },
   "source": [
    "## Передбачення та оцінювання\n",
    "\n",
    "Після навчання ми можемо використовувати модель для створення перекладів.\n",
    "\n",
    "Якщо ми перекладемо весь валідаційний набір, то зможемо скористатися [SacreBLEU](https://github.com/mjpost/sacreBLEU), щоб отримати [BLEU-оцінку](https://uk.wikipedia.org/wiki/BLEU) — найпоширеніший спосіб оцінювання якості машинного перекладу.\n",
    "\n",
    "\n",
    "### ⚠️ Важлива примітка\n",
    "\n",
    "Зазвичай SacreBLEU запускають з **командного рядка**, використовуючи згенерований файл з перекладами та оригінальний (можливо, токенізований) файл з еталонними перекладами. Це дозволяє отримати зручний рядок версії, який показує, як саме було обчислено BLEU: чи було використано нижній регістр, токенізацію (і яку саме), а також який тип згладжування використовувався.\n",
    "Якщо ви хочете дізнатися більше про правильне обчислення BLEU, радимо прочитати [цю статтю](https://arxiv.org/abs/1804.08771).\n",
    "\n",
    "Але оскільки наразі наші попередньо оброблені дані зберігаються тільки в оперативній памʼяті (і не збережені у файли), для демонстрації BLEU ми обчислимо його прямо у ноутбуці. Спершу протестуємо базову функцію BLEU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "sG7Q-aIj_jCq"
   },
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "gEj1i4do_jCq",
    "outputId": "4f288b49-6728-4f52-d66f-e396d125335a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00000000000004\n"
     ]
    }
   ],
   "source": [
    "# this should result in a perfect BLEU of 100%\n",
    "hypotheses = [\"this is a test\"]\n",
    "references = [\"this is a test\"]\n",
    "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ITRG6lpP_jCq",
    "outputId": "1ef114f6-1a43-4ddb-9359-35a44d23b06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.360679774997894\n"
     ]
    }
   ],
   "source": [
    "# here the BLEU score will be lower, because some n-grams won't match\n",
    "hypotheses = [\"this is a test\"]\n",
    "references = [\"this is a fest\"]\n",
    "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9shgZxCT_jCq"
   },
   "source": [
    "Оскільки ми виконали фільтрацію для пришвидшення, наш валідаційний набір містить 690 речень.\n",
    "Референси (еталонні переклади) — це токенізовані версії, але вони **не повинні містити токенів UNK** (тобто слів поза словником), які могла б згенерувати наша мережа.\n",
    "\n",
    "Тому ми просто беремо референси безпосередньо з об'єкта `valid_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "yHEyDXgx_jCq",
    "outputId": "8aeaeebc-1d46-427e-e8ca-da7f24a4f8fd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvalid_data\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'valid_data' is not defined"
     ]
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "CJmYkMG7_jCq",
    "outputId": "21d3909a-81a1-4e6c-cd2f-a070b8c91a3c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m references = [\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(example.trg) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalid_data\u001b[49m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(references))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(references[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'valid_data' is not defined"
     ]
    }
   ],
   "source": [
    "references = [\" \".join(example.trg) for example in valid_data]\n",
    "print(len(references))\n",
    "print(references[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "fa1ewhm1_jCq",
    "outputId": "12cd7552-0cbe-4a5d-c032-b1abf972bfb5"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mreferences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "references[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls5Sh9x-_jCq"
   },
   "source": [
    "**Now we translate the validation set!**\n",
    "\n",
    "This might take a little bit of time.\n",
    "\n",
    "Note that `greedy_decode` will cut-off the sentence when it encounters the end-of-sequence symbol, if we provide it the index of that symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZseF7Kvu_jCr"
   },
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "alphas = []  # save the last attention scores\n",
    "for batch in valid_iter:\n",
    "  batch = rebatch(PAD_INDEX, batch)\n",
    "  pred, attention = greedy_decode(\n",
    "    model, batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
    "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
    "    eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
    "  hypotheses.append(pred)\n",
    "  alphas.append(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i37OB9zj_jCr",
    "outputId": "83fef3c2-2d14-4c6c-c0b0-48ff1707345f"
   },
   "outputs": [],
   "source": [
    "# we will still need to convert the indices to actual words!\n",
    "hypotheses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug7XQ9Ky_jCr",
    "outputId": "496ba119-8f50-4230-f55b-ec2830a4cc3e"
   },
   "outputs": [],
   "source": [
    "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]\n",
    "hypotheses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blz8P2G6_jCr",
    "outputId": "e12885dc-7af8-47a3-c2ed-9a3c7e001614"
   },
   "outputs": [],
   "source": [
    "# finally, the SacreBLEU raw scorer requires string input, so we convert the lists to strings\n",
    "hypotheses = [\" \".join(x) for x in hypotheses]\n",
    "print(len(hypotheses))\n",
    "print(hypotheses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK5ge4h1_jCr",
    "outputId": "2de44578-1398-4c3e-e893-efe71cff75a5"
   },
   "outputs": [],
   "source": [
    "# now we can compute the BLEU score!\n",
    "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHnKS66M_jCr"
   },
   "source": [
    "## Attention Visualization\n",
    "\n",
    "We can also visualize the attention scores of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcg0N6yf_jCr"
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(src, trg, scores):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(scores, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels(trg, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(src, minor=False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # and the x-ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0IeZkHq_jCr",
    "outputId": "eaf96ce6-d61e-4ac7-b141-820b081d3b7a"
   },
   "outputs": [],
   "source": [
    "# This plots a chosen sentence, for which we saved the attention scores above.\n",
    "idx = 5\n",
    "src = valid_data[idx].src + [\"</s>\"]\n",
    "trg = valid_data[idx].trg + [\"</s>\"]\n",
    "pred = hypotheses[idx].split() + [\"</s>\"]\n",
    "pred_att = alphas[idx][0].T[:, :len(pred)]\n",
    "print(\"src\", src)\n",
    "print(\"ref\", trg)\n",
    "print(\"pred\", pred)\n",
    "plot_heatmap(src, pred, pred_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ksZLthR_jCr"
   },
   "source": [
    "Вітаємо! Ви завершили цей ноутбук.\n",
    "\n",
    "### Що ми **не** розглянули?\n",
    "\n",
    "* **Субслова / Byte Pair Encoding (BPE)** [\\[стаття\\]](https://arxiv.org/abs/1508.07909) [\\[репозиторій\\]](https://github.com/rsennrich/subword-nmt): дозволяють ефективно працювати з невідомими словами, розбиваючи їх на частини.\n",
    "* **Мультиплікативну / білінійну механіку уваги** можна реалізувати замість додаткової (additive) уваги, яка використовується тут [\\[стаття\\]](https://arxiv.org/abs/1508.04025).\n",
    "* Ми використали **жадібне декодування** (greedy decoding), але **beam search** дає кращі результати, розглядаючи декілька варіантів водночас.\n",
    "* **Dropout** ми використовували лише один раз у декодері. Можна експериментувати з додатковими шарами dropout: на словникових векторах, виходах енкодера тощо.\n",
    "* Можна спробувати **кілька шарів енкодера/декодера** замість одного, як у цій реалізації.\n",
    "* Для практичних експериментів радимо ознайомитися з удосконаленою системою: [Joey NMT](https://github.com/joeynmt/joeynmt) — простий, але потужний фреймворк для нейромашинного перекладу.\n",
    "\n",
    "Якщо хочеш, можемо спробувати реалізувати щось із цього разом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci_SAWgu_jCr"
   },
   "source": [
    "If this was useful to your research, please consider citing:\n",
    "\n",
    "> J Bastings. 2018. The Annotated Encoder-Decoder with Attention. https://bastings.github.io/annotated_encoder_decoder/\n",
    "\n",
    "Or use the following `Bibtex`:\n",
    "```\n",
    "@misc{bastings2018annotated,\n",
    "  title={The Annotated Encoder-Decoder with Attention},\n",
    "  author={Bastings, J.},\n",
    "  journal={https://bastings.github.io/annotated\\_encoder\\_decoder/},\n",
    "  year={2018}\n",
    "}```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
